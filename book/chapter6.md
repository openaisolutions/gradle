## Part V: The Build Is the Architecture

# Chapter 6 – The Build is the Architecture: Managing sourceSets, BOMs, and Plugins

The previous chapter focused on how to configure dependencies so your modules can compile, run and be published cleanly.  In this chapter we widen the scope and treat the **build itself as an artifact of architecture**.  A disciplined build not only drives consistent compilation and packaging, it enforces the rules that keep a monolith from descending back into chaos.  By managing shared code, defining custom source sets, adopting composite builds and publishing BOMs and SBOMs, we can encode architectural decisions directly into the build.  We also explore plugin architectures using Java’s ServiceLoader and PF4J, discuss how to generate documentation from build data and highlight security considerations.  The chapter finishes with a case study applying these techniques to our legacy application.

### Section 1: Treating the Build as Architectural Code

When you inherit a sprawling monolith, the build scripts often mirror the tangle of dependencies and ad‑hoc conventions found in the source code.  This chapter proposes viewing your Gradle build as **architectural code**: a source of truth that defines how modules depend on each other, which versions and plugins they use and what conventions they follow.  A well‑structured build enforces layering, ensures reproducible outputs and exposes architectural seams that can be exploited for refactoring.  By codifying rules in the build—such as which modules may depend on which, or which version of a library is acceptable—you shift enforcement from tribal knowledge to automated checks.  Because Gradle builds are executable programs, you can encapsulate logic in custom plugins, tasks and composite builds, reuse common settings across projects and evolve the build alongside your code.  Recognising the build as part of your architecture empowers teams to invest in it and treat changes to build files with the same rigour as production code.

### Section 2: Managing Shared Code via Dedicated Projects

Large monoliths often evolve shared helper classes that end up copied across packages.  Copy‑and‑paste reuse breeds bugs and slows down change.  A better approach is to extract common utilities into their own Gradle subprojects or libraries.  Each module that needs the shared functionality can depend on the library using the appropriate configuration (`api` if the types are part of the public API, `implementation` otherwise).  Dedicated shared projects allow you to version and publish helpers separately, centralise maintenance, add tests and gradually deprecate inline copies.  When migrating from Ant or another build system, start by identifying duplicated code—logging adapters, string utilities, IO helpers—and move them into a `common-utils` project.  Ensure the shared project has no dependencies back on consumer modules; if it does, use interfaces or events to invert the dependency direction.  Over time, the number of truly shared utilities should stabilise and the rest of the code should live in feature‑oriented modules.

### Section 3: Using Custom sourceSets

Gradle’s Java plugin defines two source sets, **main** and **test**, by convention.  However, it also allows you to define additional source sets for integration tests, functional tests or other code that should be compiled separately.  The Gradle documentation notes that you can define multiple source sets and test tasks to fully isolate each type of test in your build【280930227970882†L770-L780】.  For example, you might create a `src/integrationTest/java` directory and register an `integrationTest` source set that depends on `main` but not on unit tests.  Each custom source set has its own classpath, compilation task and tasks for running tests.  When combined with custom configurations, you can declare dependencies specific to those tests (e.g., Docker test containers) without polluting the main classpath.  Use custom source sets sparingly—only when the lifecycle is significantly different from the default.  For smaller projects, the default `test` source set is sufficient; larger codebases benefit from explicit separation of slow or external‑system tests.

### Section 4: Leveraging Composite Builds for External Modules

Gradle composite builds allow you to combine independent builds into a single workspace without flattening them into a multi‑project structure.  According to the user manual, a composite build **includes entire builds**, rather than just subprojects, and each included build is configured and executed in isolation【585526418118359†L292-L314】.  This isolation lets teams work on a library and an application together, substituting published dependencies with local projects during development【585526418118359†L292-L314】.  Composite builds are useful when your monolith depends on external libraries that you also maintain—rather than publishing a snapshot to Maven, you can `includeBuild` the library and Gradle will automatically substitute the binary dependency with the project in the included build.  You can declare included builds via the `--include-build` command‑line option for ad‑hoc substitution or permanently in `settings.gradle` using `includeBuild('path/to/lib')`【585526418118359†L424-L446】.  Composite builds keep build logic separate, avoid monolithic settings files and enable more scalable team workflows.

### Section 5: Extracting Common Utilities into Libraries

Splitting out utilities isn’t only about removing duplication; it’s also about **encapsulating responsibilities**.  Suppose you have duplicate code for parsing proprietary file formats or handling HTTP requests.  Extract these into a `parser` or `http-client` library with well‑defined APIs and no knowledge of the application’s internals.  Use Gradle’s `java-library` plugin to expose only the necessary types (`api`) while hiding implementation details (`implementation`).  Document the purpose and lifecycle of each library so other teams understand when to depend on them.  Resist the urge to place too much in a “core” module; keep libraries small and cohesive.  Over time, you may decide to publish some libraries externally or reuse them across multiple projects.  A disciplined approach to shared libraries keeps the monolith lean and reduces friction when splitting into services.

### Section 6: Creating and Using Maven BOMs

One way to centralise dependency versions across modules is to use a **Bill of Materials (BOM)**.  In Gradle, BOMs are represented by platform modules created with the `java-platform` plugin.  The dependency constraints documentation explains that you can define version constraints in a platform project’s `dependencies { constraints { ... } }` block【613331523443752†L298-L327】.  Consumer modules then declare `implementation platform(project(':my-bom'))` or `implementation platform('com.example:my-bom:1.0')` to import those versions.  Constraints can specify minimum versions (`require`) or fixed versions (`prefer`) and apply to `api` and `runtime` dependencies separately【613331523443752†L298-L327】.  Using a BOM centralises version management, reduces drift and simplifies upgrades.  When publishing a platform, remember that Maven consumers will ignore constraints unless you also publish Gradle module metadata【613331523443752†L474-L477】.  In chapter 5 we created a BOM for our modules; here we treat the BOM as part of our build architecture.

### Section 7: Publishing and Consuming Platform BOMs

After defining a BOM, you need to publish it so that other builds can consume it.  Use the `maven-publish` plugin with a `java-platform` component to publish the platform to your internal repository.  Ensure the published POM includes correct metadata (name, description, SCM coordinates) and license information.  Downstream projects import the BOM using the `platform` configuration; this ensures that all modules in the build use consistent versions, even across composite builds.  If you maintain multiple BOMs (e.g., one for platform dependencies and another for plugin versions), clearly separate their responsibilities and version them independently.  Regularly update your BOM to reflect security patches and keep a changelog so consumers know when to upgrade.  Finally, verify consumption by publishing a sample module and using `./gradlew dependencies` to confirm the imported versions.

### Section 8: Generating SBOMs with CycloneDX

Security best practices increasingly require a **Software Bill of Materials (SBOM)** to accompany each release.  The CycloneDX Gradle plugin automates SBOM generation.  The plugin portal describes that the plugin aggregates all direct and transitive dependencies of a project and produces a valid CycloneDX SBOM document【247288069817800†L52-L79】.  CycloneDX is a lightweight specification designed for human readability and machine parsing【247288069817800†L52-L55】.  To use it, apply the plugin (`id("org.cyclonedx.bom")`) and run `./gradlew cyclonedxBom`.  The plugin will create a `bom.json` or `bom.xml` file in the build directory.  You can include the SBOM in your distribution artifacts or publish it to an internal registry.  Integrating SBOM generation into your CI pipeline gives security and compliance teams the visibility they need without manual effort.  For multi‑project builds, aggregate SBOMs can be generated at the root level to cover the entire distribution.  Reviewing SBOMs also helps identify outdated or vulnerable dependencies early.

### Section 9: Enforcing Layering via Gradle Dependencies

Refactoring a monolith into modules only pays off if you enforce the boundaries you create.  One strategy is to define a **dependency graph** for your modules and ensure that the graph remains acyclic.  The Spring Modulith project, for example, provides a mechanism to verify that dependencies between modules form a directed acyclic graph and to define which modules may depend on which【311016459650721†L93-L104】.  When a new dependency violates the rules, the build fails.  In Gradle, you can codify similar rules by writing a custom task that reads the project graph (available via the `Project` API) and asserts that certain modules only depend on allowed ones.  Alternatively, adopt community plugins (such as the Kordamp Enforcer plugin) to ban unwanted dependencies or enforce convergence.  Running these checks in CI prevents accidental coupling and keeps the architecture clean.  Coupling rules evolve over time: early on you might allow UI to depend on domain and infrastructure; later you may forbid domain modules from depending on UI or external adapters.

### Section 10: Using Dependency Rules and Enforcer Plugins

Beyond verifying a clean dependency graph, you can enforce a variety of build rules using Gradle plugins.  The **Enforcer Gradle Plugin** from the Kordamp project provides a rich rule DSL to ban duplicate classes, ban specific dependencies, require certain Gradle or Java versions, and enforce dependency convergence.  For example, the `BanDuplicateClasses` rule detects when two JARs contain the same class and fails the build if found.  The `BannedDependencies` rule blocks specific modules (e.g., `commons-logging`) or version ranges.  There are also rules for requiring environment variables, enforcing bytecode versions and banning repositories【492416193534076†L0-L45】.  You apply the plugin at the root of your build and configure rules in an `enforcer { }` block.  Because rules run after project evaluation, they can analyse the resolved dependency graph and all tasks.  Use these rules to encode architectural decisions (e.g., “no module may transitively depend on log4j 1.x”) and to protect your build from misconfigurations.

### Section 11: Building a Plugin Architecture with ServiceLoader

Java’s built‑in ServiceLoader mechanism provides a simple way to discover and load implementations of interfaces at runtime.  As explained by the *Literate Java* guide, a **service** is the API you want plugins to implement and a **service provider** is the plugin JAR that provides an implementation【835708598618488†L20-L36】.  To create a plugin, you build a JAR containing a `META-INF/services` directory with a file named after the service interface listing the implementation classes【835708598618488†L50-L63】.  At runtime, your application loads implementations with `ServiceLoader.load(MyService.class)` and iterates over the discovered providers【835708598618488†L72-L79】.  ServiceLoader requires no external framework and works in standard and modular JDKs.  It is ideal for lightweight extension points, such as file format parsers or authentication strategies.  However, it offers no versioning or isolation; all plugin classes share the application’s classpath.  To avoid collisions, design your service interfaces carefully and keep plugins small.  More sophisticated plugin frameworks like PF4J build on ServiceLoader while offering isolation and lifecycle management.

### Section 12: Optional Plugins Directory and Loading Mechanisms

While ServiceLoader loads plugins from the classpath, many applications need to load plugins from an external directory.  A common pattern is to scan a configurable `plugins` folder at startup, load each JAR into its own class loader and use ServiceLoader or a custom registry to instantiate extensions.  You can implement this pattern by writing a small bootstrapping layer that registers a `URLClassLoader` for each plugin and merges resources as needed.  Store plugin metadata (name, version, dependencies) in a manifest or JSON file to validate compatibility before loading.  When combined with Gradle, you can publish each plugin as a separate project with its own `build.gradle` and use the distribution plugin to assemble them.  Provide tasks to copy plugin JARs into the `plugins` directory and integrate them into your packaging process.  A consistent plugin directory layout simplifies deployment and allows administrators to add or remove features without rebuilding the core application.

### Section 13: Implementing PF4J or Plugin Frameworks

For more complex plugin systems, consider using a dedicated framework like **PF4J**.  The PF4J project highlights that a plugin is a way for third parties to extend an application and that PF4J can transform a monolithic Java application into a modular one【520194065197238†L40-L48】.  PF4J is lightweight (around 100 KB) and loads each plugin into a separate class loader to avoid conflicts【520194065197238†L46-L52】.  It provides a `PluginManager` to load, start and stop plugins, an `ExtensionPoint` marker interface to declare extension points and an `@Extension` annotation to mark plugin implementations【520194065197238†L64-L80】.  PF4J also supports discovery of extensions via ServiceLoader or its own plugin descriptors, making it more powerful than the basic Java mechanism.  Using PF4J, you can define extension points for UI components, importers or business rules and allow third parties to provide their own implementations without changing core code.  When adopting a framework, ensure that plugin dependencies are versioned and that you have a plan for backward compatibility.

### Section 14: Exploring Java Platform Module System (JPMS)

The Java Platform Module System introduced in Java 9 enforces strong encapsulation at the language level.  Each module has a `module-info.java` file declaring which packages it exports and which modules it requires.  Gradle automatically compiles these files and passes the appropriate `--module-path` arguments to `javac` when you place them in your source tree, enabling modular builds.  Migrating an existing monolith to JPMS is a multi‑year effort: you need to ensure that packages are not split across modules, remove cyclic dependencies and decide on module names.  Start by modularising low‑level libraries and then work your way up the stack.  Tools like the `gradle-modules-plugin` can help configure the module path and test suites.  JPMS is optional for most projects but becomes attractive when you need fine‑grained encapsulation, security (only exported packages are accessible) or when using `jlink`/`jpackage` to create custom runtime images.

### Section 15: Gradually Migrating to Modular Java

Rather than switching the entire application to JPMS at once, migrate incrementally.  Adopt naming conventions for packages that map cleanly to modules, remove split packages and cycles and introduce `module-info.java` files in library modules first.  Use Gradle’s ability to mix modular and non‑modular code: modules on the module path can depend on unnamed modules on the classpath and vice versa.  As you modularise more components, tighten encapsulation by moving internal classes to non‑exported packages.  Eventually, you can enable `--illegal-access=deny` to prevent reflection on non‑exported packages and use `jlink` to build a custom runtime.  Keep in mind that some third‑party libraries may not yet support JPMS; evaluate whether the benefits outweigh the effort.  Even if you never fully adopt JPMS, the discipline of designing modules clarifies dependencies and prepares your code for future modularisation.

### Section 16: Managing Plugin Versioning and Compatibility

When you create a plugin ecosystem, you must manage plugin versioning, compatibility and upgrade strategy.  Establish semantic versioning guidelines for plugins and specify minimum compatible versions of the core application.  Use metadata in plugin manifests (e.g., `META-INF/plugin.properties`) to declare the required core version, plugin name and provider.  The plugin manager should validate this information at startup and refuse to load incompatible plugins.  Consider providing an API for plugins to access services instead of exposing internal classes.  When upgrading the core application, publish release notes highlighting breaking changes and update the plugin API accordingly.  Automated compatibility tests can load all available plugins against the new core version to detect issues early.  This discipline prevents “dependency hell” where plugins rely on conflicting versions of shared libraries.

### Section 17: Generating Documentation and Diagrams from Build Data

A build that doubles as documentation helps new developers understand the architecture.  Gradle’s **project report plugin** adds tasks that generate HTML reports showing the tasks, dependencies and properties of each project.  The documentation notes that the plugin adds tasks such as `dependencyReport`, `htmlDependencyReport`, `propertyReport` and `taskReport` and that it aggregates these into a `projectReport` task【138700670089114†L288-L330】.  The `htmlDependencyReport` produces an HTML dependency and dependency‑insight report for a project or set of projects【138700670089114†L311-L319】.  These reports mirror the information printed by `gradle dependencies` but persist them to files.  Use them to generate snapshots of the build at a point in time, document the dependency graph and identify cycles or unused dependencies.  Pair these reports with diagrams generated by tools like PlantUML or Structurizr to visualise module boundaries.  Because the reports are produced by the build itself, they are always up to date and reflect the reality of the codebase.

### Section 18: Security Considerations in Build and Plugins

Treating the build as architecture also means thinking about security.  Start by generating SBOMs with CycloneDX (see Section 8) and scanning them for known vulnerabilities.  Store SBOMs alongside release artifacts so consumers can verify the supply chain.  When writing plugins, sandbox untrusted code by loading each plugin into its own class loader (as PF4J does) and restrict reflection and file access.  Sign your plugins and verify signatures before loading them.  Keep plugin dependencies up to date by applying BOMs and enabling dependency convergence rules.  Finally, restrict the network access of build scripts—avoid downloading dependencies from untrusted repositories and configure `dependencyResolutionManagement` to use only approved registries.  A secure build pipeline protects both your code and your users.

### Section 19: Case Study – Implementing a Plugin in the Legacy App

To illustrate these concepts, consider a legacy ERP application that needs to support custom report formats.  We create a new `reports-api` module defining a `ReportGenerator` interface.  Third parties implement this interface in separate Gradle projects packaged as plugins.  Each plugin provides a manifest in `META-INF/services` listing its implementation class and includes the required dependencies in its POM.  The ERP core uses ServiceLoader to discover available generators at runtime.  To enforce boundaries, we define a platform BOM that specifies the versions of Jackson and PDF libraries and publish it to our internal Maven.  Each plugin depends on the BOM and therefore aligns on versions.  We configure the CycloneDX plugin in every plugin project to generate SBOMs and integrate the Enforcer plugin to ban outdated logging frameworks.  In CI, we run the project report plugin to produce dependency diagrams and verify that plugins don’t depend on internal ERP modules.  Over time we may migrate the API and plugins to JPMS, giving each plugin its own module name and stronger encapsulation.

### Section 20: Summary and Future Possibilities

This chapter has shown how a Gradle build can encode architecture and enforce conventions.  We discussed extracting shared code into dedicated projects, defining custom source sets for different types of code and using composite builds to work on external modules locally【585526418118359†L292-L314】.  We explored BOMs and SBOMs for version management and security【613331523443752†L298-L327】【247288069817800†L52-L79】, enforcing layering via dependency rules【311016459650721†L93-L104】, building plugin architectures using ServiceLoader【835708598618488†L50-L63】【835708598618488†L72-L79】 and PF4J【520194065197238†L40-L48】, and generating documentation from the build itself【138700670089114†L288-L330】.  By treating the build as architectural code, you gain a powerful tool for guiding refactoring, maintaining consistency and opening up your application to extension.  In the next chapter, we will address naming conventions, packaging, zipping and the logistics of delivering your application to users.