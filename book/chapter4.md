## Part III – Redefining Modules

As we continue our journey to modernise a legacy system, we turn our attention from builds and IDEs to the structure of the code itself.  In a monolithic application many *modules* are little more than namespaces, with unclear boundaries and heavy coupling.  To gain the benefits of modularity—independent development, clear interfaces and manageable complexity—we must redefine what constitutes a module.  This part explores how to group code by layers, features and business capabilities, how to break cycles and share utilities, and how to use Gradle to enforce these boundaries.  By the end of this part, we will have a practical roadmap for transforming a tangled package hierarchy into a coherent modular architecture.

### Chapter 4: Project vs Package – Redefining “Modules” in a One‑App World

Refactoring a monolith involves more than replacing Ant with Gradle—it requires reorganising code so that modules reflect how the business is structured and how teams work.  In this chapter we examine the distinctions between projects and packages, compare layered and feature‑oriented structures, and explore strategies for breaking cycles and sharing code.  We draw on lessons from microservice architecture and the Spring Modulith framework to design modules that are cohesive, loosely coupled and easy to evolve.  Each section offers practical guidance for reshaping your monolith without rewriting it from scratch.

#### Section 1: Modules Disguised as Namespaces

Legacy Java projects often claim to have dozens of modules, yet many of these “modules” are nothing more than directories under a single `src` folder.  They contain no separate build files, publish no distinct artifacts and are compiled as one unit.  What they provide is a *namespace* to avoid class name clashes.  However, a namespace is not a module.  A module encapsulates a coherent set of responsibilities behind a stable interface.  It exposes contracts to clients, hides implementation details and can evolve independently.  Without these characteristics, packages remain entangled and changes ripple throughout the system—symptoms of the “big ball of mud” anti‑pattern【289604801981996†L83-L114】.  In this section we encourage you to perform a census of your code: identify which packages truly encapsulate functionality and which simply group unrelated classes.  This inventory sets the stage for the modularisation work ahead.

#### Section 2: Layered Monolith – Web, Domain and Persistence

A common pattern in monolithic applications is the *layered monolith*, where packages represent technical layers such as web, domain and persistence.  The microservices site describes this pattern as structuring a system into a web layer for handling HTTP requests, a domain layer containing business logic, and a persistence layer for database interactions【351796604151849†L56-L63】.  Each layer depends only on the one below it: web depends on domain, which depends on persistence.  This separation fosters clarity of responsibilities and can make it easier to extract individual services later.  In our migration, we identified layers in the legacy code and evaluated whether they were truly separate.  Often, the web layer reached into persistence, or domain classes imported HTTP classes.  To cleanly adopt a layered monolith, we refactored dependencies to flow downward, extracted interfaces in the domain layer to abstract persistence, and grouped code accordingly.  While layering does not solve all modularisation issues, it provides a familiar structure and a first step away from a flat namespace.

#### Section 3: Package‑by‑Feature vs Package‑by‑Layer

Layering addresses technical separation, but many modern teams organise code by *features* rather than layers.  A note on package‑by‑feature contrasts these two approaches: package‑by‑layer places controllers, services and repositories in separate top‑level packages, whereas package‑by‑feature groups all classes related to a feature under one package【681326574613439†L70-L92】.  The top‑level structure in package‑by‑feature represents functional areas rather than technical concerns【681326574613439†L70-L92】.  This makes it easier to locate all behaviour related to, say, “Order Management” or “User Profile” and reinforces cohesion.  The same note cautions that dividing a large system into clearly separated features can be challenging, and hybrid approaches—combining feature packages at the top level with internal sub‑packages for services or repositories—may emerge【681326574613439†L104-L110】.  In our experience, package‑by‑feature works well when a feature can be developed and released independently.  When multiple features share domain concepts, layering may better communicate shared models.  We adopted a mixed strategy: high‑level packages map to business capabilities, while subpackages organise technical roles.

#### Section 4: Package‑by‑Component and Package‑by‑Module

Another variation is *package‑by‑component*, in which each component (e.g., billing, inventory, notifications) encapsulates its own controllers, services and repositories.  It is similar to package‑by‑feature but emphasises deploying components together.  Because each component owns its persistence and domain model, it can be extracted into its own module or microservice later.  Each component should expose an interface and hide its internals, as emphasised by the package‑by‑feature note【681326574613439†L70-L92】.  In Gradle, this could translate to a separate subproject for each component, with its own `build.gradle.kts` file.  We experimented with this structure by extracting our “Reporting” component from the monolith into a standalone module.  The module exported a public API for generating reports and hid the implementation behind interfaces.  This separation clarified ownership and enabled independent testing.  However, we also discovered that some components were too fine‑grained to warrant their own modules; we therefore grouped related components into a single Gradle project to avoid overhead.

#### Section 5: Grouping by Business Capability

Domain‑driven design (DDD) advocates grouping code by business capability rather than technical function.  The microservices patterns page notes that services (and by analogy modules) should be organised around business capabilities, each owned by a small team【567388280390324†L64-L74】.  Applying this principle to a monolith means identifying bounded contexts—areas of the domain with their own ubiquitous language and rules—and creating modules that align with those contexts.  For example, a commerce application might have separate modules for **Catalog**, **Orders** and **Payments**, each with its own entities, services and user interface.  When you structure the code this way, teams can work on one capability without fear of breaking another.  We used event storming workshops to map business processes and discovered natural divisions that were obscured in the old package hierarchy.  The resulting modules were cohesive and easier to reason about.

#### Section 6: Identifying Bounded Contexts

Bounded contexts are a core concept in DDD.  A bounded context defines the semantic boundary within which a particular domain model applies.  In a monolithic codebase, contexts are often intermingled: an **Order** class may be used in both the **Checkout** and **Shipping** contexts, despite having different rules.  To identify bounded contexts, we collaborated with domain experts and used mapping techniques like event storming.  We looked for areas where terminology diverged or invariants conflicted.  Once identified, we separated classes into modules according to their context.  We also avoided sharing domain objects across contexts—if two contexts need to communicate, we used value objects or events.  This isolation reduced accidental coupling and prepared the system for future extraction into services.  The package‑by‑feature note warns that dividing the system into separated features can be challenging【681326574613439†L104-L110】; bounded contexts provide a principled approach to doing so.

#### Section 7: Breaking Cyclic Dependencies with Interfaces

One of the biggest obstacles to modularisation is cyclic dependencies.  When two packages depend on each other directly, neither can be built or tested independently.  The Spring Modulith workshop notes that to break cycles you can invert dependencies by introducing an interface in the consumer module and moving the implementation to the provider【555877237476569†L63-L66】.  This technique decouples the modules: the consumer depends on the interface, while the provider implements it and supplies it via dependency injection.  In our legacy system, the **Notification** module depended on **User** to fetch email addresses, while **User** imported **Notification** classes to send welcome emails.  We resolved this by extracting a `Notifier` interface in the **User** module and having the **Notification** module implement it.  At runtime, the **User** module calls the `Notifier` without knowing the concrete implementation.  This inversion removed the cycle and allowed us to test each module independently.

#### Section 8: Extracting Shared Utilities

Another cause of cycles is shared utility code scattered across modules.  The Spring Modulith article advises moving common code out of the least central module into a shared module【555877237476569†L55-L59】.  In our project we found multiple modules implementing their own logging helpers and JSON parsers.  Rather than sharing them via copy‑and‑paste, we created a `common-util` module containing logging, date/time and configuration helpers.  Each module declared a dependency on `common-util`.  Because the helper functions were stateless and had no dependencies on business code, they did not introduce new cycles.  Centralising them improved consistency and reduced duplication.  We also adopted existing open source libraries where appropriate; there is no need to write your own JSON parsing when Jackson or Gson suffice.  The key is to avoid letting utility code become a backchannel that bypasses proper module boundaries.

#### Section 9: Using Events and Orchestrators

When inversion of control and shared modules are insufficient, you can break dependencies by moving coordination into an *orchestrator*.  The Spring Modulith workshop suggests extracting an orchestrator that depends on both modules and coordinates calls【555877237476569†L87-L91】.  Another option is to replace synchronous calls with events.  For example, rather than the **Order** module calling **Shipping** directly, the **Order** module can publish an `OrderPlaced` event.  The **Shipping** module subscribes to this event and performs its work.  This pattern eliminates the dependency from **Order** to **Shipping** and allows each module to evolve independently.  We used Spring Events and an in‑process event bus to implement this pattern within the monolith.  Eventually, these events can be externalised as messages if the modules are extracted into services.  Orchestrators and events also facilitate cross‑cutting concerns such as auditing and notifications without entangling domain logic.

#### Section 10: Merging or Splitting Modules

Not all modules should remain separate.  The Spring Modulith workshop acknowledges that sometimes the simplest solution is to merge tightly coupled modules【555877237476569†L87-L91】.  If two modules frequently change together, have reciprocal dependencies or share a domain model, splitting them may add complexity without benefit.  Conversely, a module that contains disjoint features may be a candidate for splitting.  To decide, analyse commit history and dependency graphs.  If most changes to **Catalog** also touch **Inventory**, consider merging them into a **Product** module.  We merged our **UI-Admin** and **UI-Customer** modules after realizing they shared nearly all views and controllers.  The merged module simplified cross‑feature reuse and reduced duplication.  On the other hand, we split a large **Core** module into **Authentication** and **User Profile** modules, which allowed different teams to own them.  Merging and splitting should be guided by cohesion and coupling metrics rather than arbitrary size.

#### Section 11: Using Gradle Source Sets to Structure Modules

Gradle’s flexible `sourceSets` mechanism allows you to organise code within a project without creating a new subproject.  Each module can define custom source sets for API and implementation, or for different layers of the module.  For example, you might define a `domain` source set for entities and services, and an `infra` source set for adapters to external systems.  The Java plug‑in automatically creates `main` and `test` source sets; you can add more with `sourceSets { domain { java.srcDir 'src/domain/java' } }`.  You can also configure dependencies between source sets so that `infra` can depend on `domain` but not vice versa.  This structure preserves modularity without proliferating projects.  When combined with Buildship, custom source sets appear as separate folders in your IDE.  While this technique is not explicitly part of the Spring Modulith guidance, it leverages Gradle’s capabilities to enforce boundaries and is widely used in modular monoliths.

#### Section 12: Defining Public Interfaces

For modules to interact cleanly, each must define a well‑specified public interface.  Public interfaces can be Java interfaces, Kotlin data classes, or REST APIs depending on the context.  The package‑by‑feature note emphasises that a module should expose an interface and hide its internals【681326574613439†L70-L92】.  In Gradle, you can reinforce this by placing API interfaces in a dedicated package or source set and marking implementation classes as package‑private.  For example, in our **Payments** module we defined a `PaymentService` interface with methods like `authorize` and `capture`.  The implementation class lived in an `internal` package and was not exported.  Clients depend on the interface, making it easy to replace or mock the implementation.  In a multi‑project build, you can separate API and implementation into distinct projects and depend on only the API from other modules.  Clear public interfaces foster decoupling and enable parallel development.

#### Section 13: Feature Flagging and Experimental Modules

Large refactors often involve adding new capabilities while maintaining existing behaviour.  **Feature flags** allow you to enable or disable experimental modules at runtime.  We created a `feature-flags` module that defines configuration properties controlling whether a module is active.  Modules read these flags and register or skip their beans accordingly.  This allowed us to introduce a new search engine behind a flag while preserving the old implementation.  Feature flags also provided a safety net when splitting modules: if a new module caused regressions, we could disable it and revert to the monolithic code.  Although feature flags are not specific to Gradle, they complement modularisation by decoupling deployment from release.  When combined with a build that produces separate artifacts per module, you can gradually roll out new modules to subsets of users while still building and testing them within the monolith.

#### Section 14: Cross‑Cutting Concerns: Logging, Security and Monitoring

Cross‑cutting concerns such as logging, security and monitoring span multiple modules and can erode modularity if not handled carefully.  To avoid duplicating logging code, we integrated a logging framework (SLF4J with Logback) into the `common-util` module mentioned earlier.  Modules depend on this module but do not reference specific logging implementations.  For security, we extracted authentication and authorisation into a dedicated **Security** module.  This module exposes filters and guards that other modules import, but it does not depend on domain modules.  Monitoring was implemented via aspect‑oriented programming using Spring Boot Actuator; metrics and health checks are automatically exposed for each module.  By isolating cross‑cutting concerns into their own modules or libraries, we maintained clean boundaries and reduced the risk that a feature module would inadvertently rely on another module’s internals.

#### Section 15: Testing Feature Modules in Isolation

Testing is a key benefit of modularisation.  The package‑by‑feature note emphasises that classes should be tested through the public interface of a feature or component rather than through internal classes【681326574613439†L114-L119】.  With clear module boundaries, you can write focused unit tests for each module and integration tests that exercise interactions between modules.  We created a `test-fixtures` source set per module that contained helper classes and mocks accessible only to that module’s tests.  We also configured Gradle’s `test` task to run independently for each module, enabling parallel execution.  When using interfaces to break cycles, we injected mocks of the dependencies to test modules in isolation.  Comprehensive testing within modules gave us confidence to refactor aggressively.

#### Section 16: Measuring Cohesion and Coupling

Decisions about merging or splitting modules should be guided by metrics rather than intuition.  **Cohesion** measures how closely related the responsibilities within a module are, while **coupling** measures how dependent a module is on others.  High cohesion and low coupling are hallmarks of good modularity.  To measure coupling, we generated dependency graphs using tools like Gradle’s `dependencies` and `dependencyInsight` tasks and looked for long chains of transitive dependencies.  To assess cohesion, we inspected commit history: if developers frequently modified classes across modules together, the modules were likely too coupled.  We also used static analysis tools that flag cyclic package dependencies.  Armed with these metrics, we justified merging some modules and splitting others.  Over time, we aimed to converge on modules where each class had a clear purpose and external dependencies were minimal.

#### Section 17: Migration Strategy – Converting Packages into Modules

Redefining modules is not a one‑shot activity—it’s an incremental process.  We began by creating a Gradle subproject for each candidate module and moving its classes into `src/main/java`.  We then identified its dependencies and declared them in `build.gradle.kts`.  For packages that were not yet ready to become modules, we used Gradle’s `sourceSets` to separate them logically until we could resolve their cycles.  Each time we extracted a module, we ensured that existing builds and tests remained green.  Following the Gradle migration guidance, we kept the Ant build alongside the Gradle build until both produced identical artifacts【495392568104378†L356-L371】.  We also verified that the new modules did not break the IDE experience.  As modules stabilised, we deprecated the corresponding packages in the monolithic project and documented the migration steps for the next team.  Gradual conversion allowed us to learn and adapt as we went.

#### Section 18: Case Study – Consolidating UI Modules

Our legacy system included separate **UI-Admin** and **UI-Customer** modules, each with its own controllers and templates.  Initially we planned to extract them into separate Gradle projects.  However, our coupling analysis showed that both modules reused a large portion of the same views and shared common controllers.  Nearly every commit touched both modules.  Using the principles from the Spring Modulith workshop, we decided to merge these modules【555877237476569†L87-L91】.  We created a single **UI** module with subpackages for administrative and customer functions.  To maintain separation, we defined interfaces for pages and actions that were implemented by specific subpackages.  The merger simplified navigation, reduced duplication and made it easier to apply consistent styling.  It also highlighted that our module boundaries should follow user journeys rather than arbitrary categories.  The lesson: do not hesitate to merge modules if analysis reveals they are naturally one.

#### Section 19: Case Study – Extracting Domain Services

Another example involved the **Payments** component, which handled everything from credit card authorisation to invoice generation and email notifications.  This entanglement violated the rule that a module should have a single responsibility.  We applied DDD and identified three bounded contexts: **Billing**, **Accounting** and **Notifications**.  Each context became its own module.  We used the inversion of control technique described earlier【555877237476569†L63-L66】 to decouple these modules: `Billing` defined interfaces for charging a card; `Accounting` implemented these interfaces and posted transactions to the ledger; `Notifications` subscribed to events and sent receipts.  The extraction uncovered hidden dependencies—such as a call from `Billing` to `Notifications` to update the user’s payment profile—which we replaced with events【555877237476569†L87-L91】.  Post‑extraction, each module could be built, tested and released independently.  Over time, these modules may evolve into separate services, but for now they live within the monolith, providing clear boundaries and ownership.

#### Section 20: Summary and Next Steps

In this chapter we re‑examined what it means for code to be a module.  We distinguished between mere namespaces and true modules that encapsulate responsibilities and expose clear interfaces【289604801981996†L83-L114】.  We contrasted layered monoliths with package‑by‑feature and package‑by‑component structures【351796604151849†L56-L63】【681326574613439†L70-L92】, emphasising that no single approach fits all cases.  Drawing on Domain‑Driven Design and microservices principles, we grouped modules around business capabilities【567388280390324†L64-L74】 and identified bounded contexts to delineate ownership.  To break cycles and reduce coupling we inverted dependencies, extracted shared utilities and used events and orchestrators【555877237476569†L55-L59】【555877237476569†L63-L66】【555877237476569†L87-L91】.  We explored how Gradle’s `sourceSets` and clear public interfaces reinforce boundaries, and we discussed cross‑cutting concerns, feature flags and testing strategies.

This modularisation journey is iterative.  Modules may need to be merged or split as insights emerge, and metrics of cohesion and coupling should guide decisions.  In the next chapter we will delve into dependency configurations—how `api`, `implementation`, `compileOnly` and `runtimeOnly` map to the worlds of compilation, running and publishing.  We will learn how to align Gradle’s configurations with Maven scopes and ensure that our modules can be compiled, run and consumed consistently across environments.