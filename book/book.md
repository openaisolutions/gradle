
## Part II – Elevating the Developer Experience

The second part of the book moves beyond establishing the build and begins improving the daily lives of developers.  After migrating to a Gradle monorepo, the next challenge is to integrate the build into developers’ tools and workflows.  This part explores IDE integration, debugging, continuous build and other practices that reduce friction and support rapid feedback loops.  A positive developer experience is essential to sustain momentum during a long migration; it encourages adoption, uncovers issues early and provides confidence that the refactor is heading in the right direction.

### Chapter 3: Debug or Die – Why IDE Developer Experience Still Matters

The productivity of developers is often dictated not only by the build system but by the tools they use every day.  After migrating the build, we turned our attention to smoothing the developer experience.  This chapter describes how to integrate Gradle deeply into Eclipse and IntelliJ, leverage build scans, configure run and test tasks, and provide a supportive environment for current and future team members.  A smooth workflow encourages adoption and ensures that the benefits of modularisation are realised.

#### Section 1: The Developer Experience – Feedback and Flow

Modern software development hinges on rapid feedback.  If a change breaks the build or introduces a bug, developers need to know quickly so they can adjust course.  Continuous integration practices like daily merges and automated builds reduce the risk of delivery delays and wasted integration effort【109171532237068†L115-L120】.  A responsive build pipeline frees developers to stay in a state of “flow,” moving between coding, testing and debugging without friction.  Conversely, a clumsy toolchain forces context switches and hampers productivity.

In our project, moving to Gradle eliminated many manual steps, but to fully benefit we had to ensure that IDE tasks, debuggers and tests worked seamlessly.  We set a goal that developers should be able to clone the repository, run a single command to import the project into their favourite IDE, and start coding without editing configuration files.  Achieving this goal required understanding the capabilities of each IDE and configuring Gradle to cooperate with them.  Throughout this chapter we will highlight how to preserve the intuitive flow of development while improving build fidelity.

#### Section 2: IDE Landscape – Eclipse, IntelliJ and NetBeans

Java developers typically rely on rich integrated development environments such as Eclipse, IntelliJ IDEA and NetBeans.  Each IDE offers features like code completion, refactoring tools and test runners, but their Gradle integration varies.  Eclipse uses the **Buildship** plug‑in to integrate Gradle projects; Buildship is a collection of Eclipse plug‑ins that provides deep integration with Gradle, allowing you to execute tasks, inspect dependencies and synchronise the IDE model with the build【56816376473026†L239-L244】.  IntelliJ IDEA ships with its own Gradle importer and run configuration interface that reads the build script and generates appropriate project settings.  NetBeans also includes a Gradle support plug‑in that recognises multi‑project builds and displays tasks in a sidebar.

While migrating, we tested all three IDEs to ensure that the build worked consistently.  Eclipse and IntelliJ proved the most robust; Buildship’s ability to work directly with the Gradle Tooling API means that no manual generation of `.classpath` files is required.  IntelliJ’s importer similarly recognises `build.gradle` and `settings.gradle` and provides contextual actions for running tasks.  Regardless of IDE, our priority was to avoid any IDE‑specific hacks in the build scripts.  By targeting Gradle’s standard configuration, we allowed developers to choose their preferred tool while maintaining a single source of truth for dependencies and tasks.

#### Section 3: Gradle Buildship Integration

Eclipse users previously relied on the `eclipse` Gradle plug‑in to generate `.project` and `.classpath` files.  In the Gradle era we switched to Buildship, which interacts with Gradle through the Tooling API.  Buildship is described as a set of Eclipse plug‑ins that provides deep integration with Gradle【56816376473026†L239-L244】.  The Gradle user manual echoes this, calling Buildship a plug‑in collection that embeds Gradle into Eclipse【657788656503172†L330-L341】.  Once installed, Buildship offers a “Gradle Tasks” view where you can browse tasks for each subproject, run them directly and view console output.

To import a project, choose **File → Import → Gradle → Existing Gradle Project** and select the root directory.  Buildship reads `settings.gradle`, detects all subprojects and configures the workspace accordingly.  When you change `build.gradle` or `settings.gradle`, Buildship prompts you to synchronise the project; this reloads the build model and updates classpaths and project settings.  Unlike the old `eclipse` tasks, Buildship does not persist generated files in version control.  Instead it treats Gradle as the canonical source of configuration.  For teams migrating from Ant this is a major improvement: there is no need to commit `.classpath` tweaks, and the IDE reflects the build exactly.

#### Section 4: Synchronising Sources and Classpaths

During the migration we encountered issues with stale classpaths and missing source folders when developers manually edited `.classpath` files.  Buildship resolves this by deriving the classpath directly from Gradle’s dependency graph.  When you refresh the project, Buildship analyses `sourceSets`, identifies `src/main/java`, `src/main/resources` and any custom directories, and updates the Eclipse project accordingly.  Similarly, IntelliJ re‑reads the Gradle model and updates module settings.  To synchronise the project, either accept Buildship’s prompt after editing build files or right‑click the project and choose **Gradle → Refresh Gradle Project**.  This ensures that new modules, dependencies and source sets appear in the IDE without manual changes.

If your build uses unconventional source directories, specify them in `sourceSets` in `build.gradle`.  For example:

```
sourceSets {
    main {
        java.srcDirs = ['src']
        resources.srcDirs = ['resources']
    }
}
```

Defining source sets declaratively allows Buildship and IntelliJ to configure the IDE correctly.  As you move modules from Ant layouts to Gradle conventions, update these declarations to reflect the new structure.

#### Section 5: Debugging Across Modules

Legacy systems often consist of multiple modules that must be launched together for debugging.  IntelliJ IDEA offers a Gradle run configuration that can execute tasks across modules and attach the debugger.  A useful option is **Debug all tasks on the execution graph**, which instructs the IDE to start the JVM in debug mode for every Gradle task【86014017228282†L326-L361】.  This allows you to set breakpoints in any module and inspect variables as the application starts.  Alternatively, you can uncheck this option and debug only tasks that run tests or your application, reducing overhead【86014017228282†L326-L361】.

For Eclipse users, Buildship exposes the `run` task of the application plug‑in and any custom tasks.  You can right‑click the `run` task and choose **Debug** to start the application with a debugger attached.  If a module uses the Java Library plug‑in rather than the application plug‑in, create a custom task that invokes `JavaExec` and configure its `debug` property.  When modules depend on each other, ensure that their `implementation` relationships are correctly defined so that the debugger can resolve symbols across boundaries.  By standardising on Gradle tasks for launching the application, we avoided maintaining separate IDE launchers and ensured consistent behaviour across development and CI.

#### Section 6: Running Gradle Tasks from the IDE

IntelliJ and Eclipse both support running arbitrary Gradle tasks from within the IDE.  In IntelliJ you can create a **Run/Debug configuration** of type **Gradle** and specify the tasks and arguments to execute【86014017228282†L12-L50】.  You might set the task field to `clean build` or `:moduleA:test` and pass JVM arguments under the **VM options** field.  Configurations can be stored in the `.idea` directory or shared via version control.  For repetitive tasks like assembling the distribution or generating documentation, save separate configurations to avoid typing long commands.

Eclipse’s Buildship provides a similar “Run Configurations” dialog.  Selecting a Gradle project allows you to choose tasks, specify command‑line options and set environment variables.  The configuration persists within the workspace but is not checked into the repository.  Buildship also includes a **Tasks** view where double‑clicking a task executes it with default arguments.  This integration reduces the need to switch to a terminal and ensures that tasks are executed with the correct wrapper version and project directory.

#### Section 7: Configuring Source Sets for IDEs

As you modularise the application, some modules may use non‑standard source layouts.  For example, a module migrated from Ant might place Java code under `src` rather than `src/main/java`.  Gradle’s `sourceSets` mechanism allows you to map these directories so that the IDE recognises them.  Declare custom source sets in your `build.gradle` file and ensure that each has a unique `name`.  Buildship and IntelliJ read these definitions and create corresponding folders in the project explorer.  If your module contains generated sources, specify them in `sourceSets.main.java.srcDirs` and mark them as generated sources in IntelliJ to avoid editing warnings.

Over time you should move toward Gradle’s conventional layout, but during migration, explicit `sourceSets` declarations preserve the existing structure.  Remember to update integration tests or integration resources directories as well.  Consistently defining source sets prevents confusion about where code should live and ensures that IDE features such as search, refactoring and code generation work reliably.

#### Section 8: Managing Duplicate Classes and Classpaths

One frequent problem when migrating from Ant to Gradle is duplicated entries in the IDE classpath.  In a forum thread a user discovered that running `gradlew cleanEclipse eclipse` caused duplicate dependency entries to appear in the `.classpath` file and break compilation【929742176678335†L58-L71】.  The recommended workaround was to remove duplicates during the `.classpath` generation.  The script uses the `whenMerged` hook to group classpath entries by their path and then re‑insert only unique entries【929742176678335†L158-L179】.  In Gradle you can add this logic to the `eclipseClasspath` configuration to eliminate duplicates automatically.

Switching to Buildship largely sidesteps this issue because the IDE derives the classpath directly from the dependency graph.  However, if you still need to generate Eclipse files—for example, for teams not using Buildship—apply the duplicate removal pattern.  Also check for duplicate classes within your modules.  When two modules define the same package and class name, the IDE might compile one and run another, causing unpredictable behaviour.  Use your dependency graph visualisation and static analysis tools to detect and eliminate duplicates early.

#### Section 9: Resolving Source Lookup Failures

Another issue encountered during migration is the “source not found” error when debugging third‑party libraries.  A Gradle forum discussion explained that using a `flatDir` repository prevents Buildship from finding source JARs because the repository has no group or artifact coordinates【268312600825314†L60-L72】.  The advice is to avoid `flatDir` repositories and instead structure local repositories in a Maven‑like layout (`group/artifact/version`), enabling the IDE to download sources and javadocs automatically【268312600825314†L169-L172】.  If you must use local JARs temporarily, add them via `implementation files('libs/foo.jar')` and accept that source lookup will not work until the dependency is published properly.

To ensure good debugging support, prefer declaring dependencies via Maven Central or your internal repository.  When migrating, plan to migrate proprietary libraries into a repository so that Gradle and the IDE can locate sources.  For external libraries, enable the “Download Sources” option in IntelliJ’s Gradle settings.  Buildship fetches sources automatically when they are available.  Removing `flatDir` dependencies was one of the first tasks we undertook to improve developer experience.

#### Section 10: Adjusting Build Directories

In legacy builds, compiled classes and scripts often live in a `bin` directory.  Gradle uses a different convention: compiled classes are placed under `build/classes` and distribution scripts are generated under `build/distributions`.  If your application depends on a particular directory structure—perhaps because an Ant script expects to find executables in `bin`—you can customise Gradle’s output.  The Application plug‑in exposes an `executableDir` property that controls where start scripts are generated【97055485946526†L353-L377】.  For example, setting `applicationDistribution.from installDist.executableDir = new File(project.buildDir, 'bin')` will produce scripts in the `build/bin` directory.

Another approach is to update downstream scripts to look in Gradle’s default locations.  In our migration we first customised the distribution directory to maintain compatibility with existing launchers, then gradually removed those launchers as we replaced them with `gradlew run` commands.  This allowed the team to continue working while we refactored the deployment pipeline.  When adjusting build directories, document your changes and ensure that continuous integration scripts and packaging tasks refer to the new paths.

#### Section 11: Custom Run Configurations

Developers often need to pass additional arguments or environment variables when running a module.  Both IntelliJ and Eclipse allow you to create custom run configurations.  In IntelliJ’s **Run/Debug** dialog you can specify tasks such as `run` and add program arguments or JVM options in the respective fields【86014017228282†L12-L50】.  You can also choose whether to run tasks incrementally or always run them, and save the configuration so that it can be shared across the team.  For instance, a configuration might run `:moduleA:run` with `--args='--profile=dev'` and a custom heap size.

Eclipse’s run configuration interface for Gradle provides similar options.  You can select multiple tasks, specify `--debug-jvm` to run the application in debug mode and set environment variables.  If you need to debug build scripts themselves, Buildship offers a setting to enable script debugging, but this can slow down execution.  Disabling script debugging will debug only the executed Java application or tests【86014017228282†L326-L361】.  For reproducibility, avoid embedding absolute paths or secrets in run configurations—use project properties or environment variables instead.

#### Section 12: Using the Application Plugin

When your project contains an application, Gradle’s application plug‑in simplifies running and packaging it.  The plug‑in implicitly applies the Java and distribution plug‑ins and adds tasks such as `run` and `distZip`【97055485946526†L291-L350】.  You define the fully qualified name of your main class using the `mainClass` property, and Gradle generates scripts under `build/install/<project>` to start the application.  Running `./gradlew run` executes your application locally, and you can pass arguments using `--args`, for example `./gradlew run --args='--port=8080'`【97055485946526†L291-L350】.

The plug‑in also supports debugging.  Passing `--debug-jvm` to the `run` task instructs Gradle to start the JVM in debug mode, allowing you to attach a debugger from your IDE【97055485946526†L291-L350】.  You can customise the default JVM arguments by configuring the `applicationDefaultJvmArgs` property or modify the generated start scripts via `distributions { main { contents.from(...) } }`.  By relying on the application plug‑in, we replaced custom Ant launchers with a standard approach that works consistently across environments.

#### Section 13: Testing Frameworks in the IDE

Gradle supports various testing frameworks such as JUnit, TestNG and Spock.  In IntelliJ you can choose whether to use the IDE’s built‑in test runner or Gradle’s test runner.  JetBrains documentation explains that you can set the **Test Runner** to Gradle, IntelliJ or select per test class【758756572275466†L20-L69】.  Running tests with the Gradle test runner ensures that the behaviour matches the command line and continuous integration.  The IDE displays test results in a dedicated tool window, showing passed, failed and skipped tests.  You can right‑click a test class or method in the editor and choose **Run Gradle Test** to execute it.

Debugging tests is equally straightforward.  Choosing **Debug Gradle Test** attaches the debugger to the test process.  If you prefer the IntelliJ test runner, you can run tests without Gradle, but the results may differ from CI if build scripts define custom test tasks.  In Eclipse, Buildship surfaces the `test` task, and running it opens the JUnit view.  To run a single test class, use `gradlew :module:test --tests com.example.MyClass`.  Adopting consistent test runners across IDE and CI avoids surprise failures.

#### Section 14: Hot Reload and Continuous Build

Developers appreciate immediate feedback when editing code.  Gradle offers **continuous build** mode, which re‑executes tasks automatically when their inputs change.  The manual describes running tasks with the `--continuous` flag; Gradle starts watching the file system and reruns the task whenever inputs change【908571624651963†L287-L387】.  This feature is invaluable for tasks such as compiling classes or regenerating documentation.  However, the documentation warns that continuous build is not integrated into IDE workflows—you must run `./gradlew <task> --continuous` in a separate terminal window alongside your IDE【908571624651963†L287-L387】.  IDEs will reflect changes once the task completes.

Some frameworks offer true hot reload, where the running application automatically reloads classes without restarting the JVM.  Combining Gradle’s continuous build with the application plug‑in can approximate this: one process rebuilds the classes, while another monitors the output and reloads classes.  For example, Spring Boot’s devtools watch for changes in `build/classes` and restart the application.  When using continuous build, ensure that tasks are idempotent and quick to run; long‑running tasks can consume CPU and hinder performance.  Over time we expect IDEs to integrate file watching more tightly, but until then, continuous build remains a valuable tool.

#### Section 15: Leveraging Build Scans

Despite best efforts, builds sometimes fail unexpectedly.  Gradle’s build scan service provides a powerful way to diagnose such failures.  The features page explains that a build scan captures a wealth of information about your build and publishes it to a web application【856209582615570†L156-L168】.  Build scans include details such as build environment, task execution order, dependency resolution, and test results.  You can share the scan URL with colleagues or support teams, enabling collaborative debugging【856209582615570†L156-L168】.  Build scans also allow you to compare two builds to identify differences, making it easier to pinpoint regressions.

To generate a build scan, add the `com.gradle.build-scan` plug‑in to your build and run `./gradlew build --scan`.  Accept the terms of service when prompted, and Gradle will upload the scan to the Gradle Enterprise server.  In many cases, the free public server is sufficient for open source projects.  For commercial projects, consider hosting your own server to retain control of build data.  Build scans became part of our migration toolkit; when a developer reported a mysterious failure, we asked for a scan and quickly identified issues such as incompatible Java versions or misconfigured proxies.  The ability to visualise the dependency graph and task execution timeline in the browser accelerated troubleshooting.

#### Section 16: Developer Feedback Loops

Establishing a tight feedback loop is essential for maintaining momentum during long migrations.  Martin Fowler notes that continuous integration reduces the risk of delivery delays and wasted integration effort by merging changes frequently and running automated tests【109171532237068†L115-L120】.  A healthy feedback loop involves quick local builds, prompt test results and fast debugging cycles.  To achieve this, we invested in a powerful CI server that executed the build on every push and provided notifications via chat.  Developers could run the same tasks locally with `./gradlew build` or via their IDE run configurations.  For integration tests that took longer, we separated them into dedicated tasks so that the fast feedback loop remained unblocked.

In the IDE, Buildship and IntelliJ’s Gradle integration allowed developers to run tasks incrementally and observe results in context.  We encouraged developers to commit small changes frequently and rely on the build to catch regressions.  Combined with build scans, this practice built confidence that the system remained stable as modules were extracted and refactored.  Continuous integration thus served both as a safety net and as a forcing function for modularity: you cannot have a green build if cycles and missing dependencies linger.

#### Section 17: Linting and Static Analysis

Quality checks are an important part of developer experience.  Gradle’s Checkstyle plug‑in performs static analysis on Java source files according to a configurable set of rules.  The user guide notes that the plug‑in adds tasks like `checkstyleMain` and `checkstyleTest`, which are executed when you run `gradle check`【904677006638313†L292-L318】.  The plug‑in integrates with Gradle’s Java plug‑in, ensuring that checks run with the same Java version used by the project【904677006638313†L352-L365】.  The default configuration files live under `config/checkstyle`, but you can point the plug‑in at your own `google_checks.xml` or `sun_checks.xml` to enforce your coding standard.

In the IDE, Checkstyle can be enabled via plug‑ins for Eclipse and IntelliJ.  Buildship automatically adds the Checkstyle tasks to the Gradle view; right‑clicking `checkstyleMain` runs the checks and displays the results.  IntelliJ’s `Checkstyle-IDEA` plug‑in presents violations inline in the editor.  Beyond Checkstyle, we adopted additional static analysis tools such as SpotBugs and PMD, configured through their respective Gradle plug‑ins.  Running these tools in CI and locally helps catch bugs and maintain coding standards, improving the overall quality of the codebase.

#### Section 18: Troubleshooting Common IDE Issues

Even with a modern build, IDE integration can occasionally break down.  Duplicate classpath entries, discussed earlier, can cause build failures; ensure that you do not mix `eclipse` generation tasks with Buildship and remove stale `.classpath` files.  If the IDE cannot find sources for dependencies, check whether you are using `flatDir` repositories and replace them with proper Maven repositories【268312600825314†L60-L72】.  When builds fail before any tasks execute, network misconfiguration can be the culprit—Gradle daemon logs may reveal that the build cannot contact remote repositories due to firewall or proxy issues【411864422224135†L471-L496】.  Also, check that your IDE uses the same Java version as Gradle; mismatches can lead to compiler errors.

Occasionally, IntelliJ will mark code as red even though it compiles from the command line.  In such cases, invalidate the IDE caches and reimport the project.  For Eclipse, if Buildship displays stale data, delete the `.gradle` directory in the workspace and refresh the project.  Keep an eye on plug‑in versions—upgrading Buildship or the IntelliJ Gradle plug‑in can resolve integration bugs.  By documenting these troubleshooting steps and sharing them with the team, we minimized downtime and frustration.

#### Section 19: Onboarding New Developers

An often overlooked aspect of developer experience is onboarding.  Bringing new engineers up to speed quickly is crucial for retaining talent and maintaining velocity.  An article on developer onboarding emphasises that onboarding integrates new developers into the team and provides them with knowledge, resources and relationships【735336400211954†L66-L101】.  Companies with effective onboarding see 62 % greater productivity from new hires and 50 % greater retention【735336400211954†L66-L101】.  The article further explains that a structured program gives newcomers a sense of belonging and accelerates their ability to contribute【735336400211954†L96-L119】.

To support onboarding in our migration project, we created a wiki page outlining how to clone the repository, run the build and import the project into the IDE of their choice.  We included links to Buildship and IntelliJ documentation, explained common pitfalls (e.g., do not use `flatDir` repositories), and provided sample run configurations.  We also paired new developers with experienced team members and hosted regular brown bag sessions to explain the architecture and migration goals.  By investing in onboarding, we ensured that new contributors could quickly become productive and contribute to the migration effort.

#### Section 20: Summary and Insights

This chapter highlighted the importance of a positive developer experience when refactoring a legacy application.  We explored how IDE integration via Buildship and IntelliJ simplifies running, testing and debugging multi‑module Gradle projects.  We discussed configuring source sets, managing classpaths, customising run configurations and leveraging Gradle’s application plug‑in.  We emphasised the value of continuous build and build scans for fast feedback and collaborative troubleshooting【856209582615570†L156-L168】【908571624651963†L287-L387】.  We also underscored the role of quality tools like Checkstyle and the necessity of good onboarding practices【904677006638313†L292-L318】【735336400211954†L66-L101】.

Ultimately, a pleasant developer experience is not just a perk—it is a prerequisite for successful modernisation.  When tools work with you rather than against you, developers spend less time fighting the environment and more time delivering value.  With the build system stabilised and the IDE workflow streamlined, the next chapters can focus on aligning dependency configurations, enforcing architectural boundaries and packaging the application for delivery.

## Part III – Redefining Modules

As we continue our journey to modernise a legacy system, we turn our attention from builds and IDEs to the structure of the code itself.  In a monolithic application many *modules* are little more than namespaces, with unclear boundaries and heavy coupling.  To gain the benefits of modularity—independent development, clear interfaces and manageable complexity—we must redefine what constitutes a module.  This part explores how to group code by layers, features and business capabilities, how to break cycles and share utilities, and how to use Gradle to enforce these boundaries.  By the end of this part, we will have a practical roadmap for transforming a tangled package hierarchy into a coherent modular architecture.

### Chapter 4: Project vs Package – Redefining “Modules” in a One‑App World

Refactoring a monolith involves more than replacing Ant with Gradle—it requires reorganising code so that modules reflect how the business is structured and how teams work.  In this chapter we examine the distinctions between projects and packages, compare layered and feature‑oriented structures, and explore strategies for breaking cycles and sharing code.  We draw on lessons from microservice architecture and the Spring Modulith framework to design modules that are cohesive, loosely coupled and easy to evolve.  Each section offers practical guidance for reshaping your monolith without rewriting it from scratch.

#### Section 1: Modules Disguised as Namespaces

Legacy Java projects often claim to have dozens of modules, yet many of these “modules” are nothing more than directories under a single `src` folder.  They contain no separate build files, publish no distinct artifacts and are compiled as one unit.  What they provide is a *namespace* to avoid class name clashes.  However, a namespace is not a module.  A module encapsulates a coherent set of responsibilities behind a stable interface.  It exposes contracts to clients, hides implementation details and can evolve independently.  Without these characteristics, packages remain entangled and changes ripple throughout the system—symptoms of the “big ball of mud” anti‑pattern【289604801981996†L83-L114】.  In this section we encourage you to perform a census of your code: identify which packages truly encapsulate functionality and which simply group unrelated classes.  This inventory sets the stage for the modularisation work ahead.

#### Section 2: Layered Monolith – Web, Domain and Persistence

A common pattern in monolithic applications is the *layered monolith*, where packages represent technical layers such as web, domain and persistence.  The microservices site describes this pattern as structuring a system into a web layer for handling HTTP requests, a domain layer containing business logic, and a persistence layer for database interactions【351796604151849†L56-L63】.  Each layer depends only on the one below it: web depends on domain, which depends on persistence.  This separation fosters clarity of responsibilities and can make it easier to extract individual services later.  In our migration, we identified layers in the legacy code and evaluated whether they were truly separate.  Often, the web layer reached into persistence, or domain classes imported HTTP classes.  To cleanly adopt a layered monolith, we refactored dependencies to flow downward, extracted interfaces in the domain layer to abstract persistence, and grouped code accordingly.  While layering does not solve all modularisation issues, it provides a familiar structure and a first step away from a flat namespace.

#### Section 3: Package‑by‑Feature vs Package‑by‑Layer

Layering addresses technical separation, but many modern teams organise code by *features* rather than layers.  A note on package‑by‑feature contrasts these two approaches: package‑by‑layer places controllers, services and repositories in separate top‑level packages, whereas package‑by‑feature groups all classes related to a feature under one package【681326574613439†L70-L92】.  The top‑level structure in package‑by‑feature represents functional areas rather than technical concerns【681326574613439†L70-L92】.  This makes it easier to locate all behaviour related to, say, “Order Management” or “User Profile” and reinforces cohesion.  The same note cautions that dividing a large system into clearly separated features can be challenging, and hybrid approaches—combining feature packages at the top level with internal sub‑packages for services or repositories—may emerge【681326574613439†L104-L110】.  In our experience, package‑by‑feature works well when a feature can be developed and released independently.  When multiple features share domain concepts, layering may better communicate shared models.  We adopted a mixed strategy: high‑level packages map to business capabilities, while subpackages organise technical roles.

#### Section 4: Package‑by‑Component and Package‑by‑Module

Another variation is *package‑by‑component*, in which each component (e.g., billing, inventory, notifications) encapsulates its own controllers, services and repositories.  It is similar to package‑by‑feature but emphasises deploying components together.  Because each component owns its persistence and domain model, it can be extracted into its own module or microservice later.  Each component should expose an interface and hide its internals, as emphasised by the package‑by‑feature note【681326574613439†L70-L92】.  In Gradle, this could translate to a separate subproject for each component, with its own `build.gradle.kts` file.  We experimented with this structure by extracting our “Reporting” component from the monolith into a standalone module.  The module exported a public API for generating reports and hid the implementation behind interfaces.  This separation clarified ownership and enabled independent testing.  However, we also discovered that some components were too fine‑grained to warrant their own modules; we therefore grouped related components into a single Gradle project to avoid overhead.

#### Section 5: Grouping by Business Capability

Domain‑driven design (DDD) advocates grouping code by business capability rather than technical function.  The microservices patterns page notes that services (and by analogy modules) should be organised around business capabilities, each owned by a small team【567388280390324†L64-L74】.  Applying this principle to a monolith means identifying bounded contexts—areas of the domain with their own ubiquitous language and rules—and creating modules that align with those contexts.  For example, a commerce application might have separate modules for **Catalog**, **Orders** and **Payments**, each with its own entities, services and user interface.  When you structure the code this way, teams can work on one capability without fear of breaking another.  We used event storming workshops to map business processes and discovered natural divisions that were obscured in the old package hierarchy.  The resulting modules were cohesive and easier to reason about.

#### Section 6: Identifying Bounded Contexts

Bounded contexts are a core concept in DDD.  A bounded context defines the semantic boundary within which a particular domain model applies.  In a monolithic codebase, contexts are often intermingled: an **Order** class may be used in both the **Checkout** and **Shipping** contexts, despite having different rules.  To identify bounded contexts, we collaborated with domain experts and used mapping techniques like event storming.  We looked for areas where terminology diverged or invariants conflicted.  Once identified, we separated classes into modules according to their context.  We also avoided sharing domain objects across contexts—if two contexts need to communicate, we used value objects or events.  This isolation reduced accidental coupling and prepared the system for future extraction into services.  The package‑by‑feature note warns that dividing the system into separated features can be challenging【681326574613439†L104-L110】; bounded contexts provide a principled approach to doing so.

#### Section 7: Breaking Cyclic Dependencies with Interfaces

One of the biggest obstacles to modularisation is cyclic dependencies.  When two packages depend on each other directly, neither can be built or tested independently.  The Spring Modulith workshop notes that to break cycles you can invert dependencies by introducing an interface in the consumer module and moving the implementation to the provider【555877237476569†L63-L66】.  This technique decouples the modules: the consumer depends on the interface, while the provider implements it and supplies it via dependency injection.  In our legacy system, the **Notification** module depended on **User** to fetch email addresses, while **User** imported **Notification** classes to send welcome emails.  We resolved this by extracting a `Notifier` interface in the **User** module and having the **Notification** module implement it.  At runtime, the **User** module calls the `Notifier` without knowing the concrete implementation.  This inversion removed the cycle and allowed us to test each module independently.

#### Section 8: Extracting Shared Utilities

Another cause of cycles is shared utility code scattered across modules.  The Spring Modulith article advises moving common code out of the least central module into a shared module【555877237476569†L55-L59】.  In our project we found multiple modules implementing their own logging helpers and JSON parsers.  Rather than sharing them via copy‑and‑paste, we created a `common-util` module containing logging, date/time and configuration helpers.  Each module declared a dependency on `common-util`.  Because the helper functions were stateless and had no dependencies on business code, they did not introduce new cycles.  Centralising them improved consistency and reduced duplication.  We also adopted existing open source libraries where appropriate; there is no need to write your own JSON parsing when Jackson or Gson suffice.  The key is to avoid letting utility code become a backchannel that bypasses proper module boundaries.

#### Section 9: Using Events and Orchestrators

When inversion of control and shared modules are insufficient, you can break dependencies by moving coordination into an *orchestrator*.  The Spring Modulith workshop suggests extracting an orchestrator that depends on both modules and coordinates calls【555877237476569†L87-L91】.  Another option is to replace synchronous calls with events.  For example, rather than the **Order** module calling **Shipping** directly, the **Order** module can publish an `OrderPlaced` event.  The **Shipping** module subscribes to this event and performs its work.  This pattern eliminates the dependency from **Order** to **Shipping** and allows each module to evolve independently.  We used Spring Events and an in‑process event bus to implement this pattern within the monolith.  Eventually, these events can be externalised as messages if the modules are extracted into services.  Orchestrators and events also facilitate cross‑cutting concerns such as auditing and notifications without entangling domain logic.

#### Section 10: Merging or Splitting Modules

Not all modules should remain separate.  The Spring Modulith workshop acknowledges that sometimes the simplest solution is to merge tightly coupled modules【555877237476569†L87-L91】.  If two modules frequently change together, have reciprocal dependencies or share a domain model, splitting them may add complexity without benefit.  Conversely, a module that contains disjoint features may be a candidate for splitting.  To decide, analyse commit history and dependency graphs.  If most changes to **Catalog** also touch **Inventory**, consider merging them into a **Product** module.  We merged our **UI-Admin** and **UI-Customer** modules after realizing they shared nearly all views and controllers.  The merged module simplified cross‑feature reuse and reduced duplication.  On the other hand, we split a large **Core** module into **Authentication** and **User Profile** modules, which allowed different teams to own them.  Merging and splitting should be guided by cohesion and coupling metrics rather than arbitrary size.

#### Section 11: Using Gradle Source Sets to Structure Modules

Gradle’s flexible `sourceSets` mechanism allows you to organise code within a project without creating a new subproject.  Each module can define custom source sets for API and implementation, or for different layers of the module.  For example, you might define a `domain` source set for entities and services, and an `infra` source set for adapters to external systems.  The Java plug‑in automatically creates `main` and `test` source sets; you can add more with `sourceSets { domain { java.srcDir 'src/domain/java' } }`.  You can also configure dependencies between source sets so that `infra` can depend on `domain` but not vice versa.  This structure preserves modularity without proliferating projects.  When combined with Buildship, custom source sets appear as separate folders in your IDE.  While this technique is not explicitly part of the Spring Modulith guidance, it leverages Gradle’s capabilities to enforce boundaries and is widely used in modular monoliths.

#### Section 12: Defining Public Interfaces

For modules to interact cleanly, each must define a well‑specified public interface.  Public interfaces can be Java interfaces, Kotlin data classes, or REST APIs depending on the context.  The package‑by‑feature note emphasises that a module should expose an interface and hide its internals【681326574613439†L70-L92】.  In Gradle, you can reinforce this by placing API interfaces in a dedicated package or source set and marking implementation classes as package‑private.  For example, in our **Payments** module we defined a `PaymentService` interface with methods like `authorize` and `capture`.  The implementation class lived in an `internal` package and was not exported.  Clients depend on the interface, making it easy to replace or mock the implementation.  In a multi‑project build, you can separate API and implementation into distinct projects and depend on only the API from other modules.  Clear public interfaces foster decoupling and enable parallel development.

#### Section 13: Feature Flagging and Experimental Modules

Large refactors often involve adding new capabilities while maintaining existing behaviour.  **Feature flags** allow you to enable or disable experimental modules at runtime.  We created a `feature-flags` module that defines configuration properties controlling whether a module is active.  Modules read these flags and register or skip their beans accordingly.  This allowed us to introduce a new search engine behind a flag while preserving the old implementation.  Feature flags also provided a safety net when splitting modules: if a new module caused regressions, we could disable it and revert to the monolithic code.  Although feature flags are not specific to Gradle, they complement modularisation by decoupling deployment from release.  When combined with a build that produces separate artifacts per module, you can gradually roll out new modules to subsets of users while still building and testing them within the monolith.

#### Section 14: Cross‑Cutting Concerns: Logging, Security and Monitoring

Cross‑cutting concerns such as logging, security and monitoring span multiple modules and can erode modularity if not handled carefully.  To avoid duplicating logging code, we integrated a logging framework (SLF4J with Logback) into the `common-util` module mentioned earlier.  Modules depend on this module but do not reference specific logging implementations.  For security, we extracted authentication and authorisation into a dedicated **Security** module.  This module exposes filters and guards that other modules import, but it does not depend on domain modules.  Monitoring was implemented via aspect‑oriented programming using Spring Boot Actuator; metrics and health checks are automatically exposed for each module.  By isolating cross‑cutting concerns into their own modules or libraries, we maintained clean boundaries and reduced the risk that a feature module would inadvertently rely on another module’s internals.

#### Section 15: Testing Feature Modules in Isolation

Testing is a key benefit of modularisation.  The package‑by‑feature note emphasises that classes should be tested through the public interface of a feature or component rather than through internal classes【681326574613439†L114-L119】.  With clear module boundaries, you can write focused unit tests for each module and integration tests that exercise interactions between modules.  We created a `test-fixtures` source set per module that contained helper classes and mocks accessible only to that module’s tests.  We also configured Gradle’s `test` task to run independently for each module, enabling parallel execution.  When using interfaces to break cycles, we injected mocks of the dependencies to test modules in isolation.  Comprehensive testing within modules gave us confidence to refactor aggressively.

#### Section 16: Measuring Cohesion and Coupling

Decisions about merging or splitting modules should be guided by metrics rather than intuition.  **Cohesion** measures how closely related the responsibilities within a module are, while **coupling** measures how dependent a module is on others.  High cohesion and low coupling are hallmarks of good modularity.  To measure coupling, we generated dependency graphs using tools like Gradle’s `dependencies` and `dependencyInsight` tasks and looked for long chains of transitive dependencies.  To assess cohesion, we inspected commit history: if developers frequently modified classes across modules together, the modules were likely too coupled.  We also used static analysis tools that flag cyclic package dependencies.  Armed with these metrics, we justified merging some modules and splitting others.  Over time, we aimed to converge on modules where each class had a clear purpose and external dependencies were minimal.

#### Section 17: Migration Strategy – Converting Packages into Modules

Redefining modules is not a one‑shot activity—it’s an incremental process.  We began by creating a Gradle subproject for each candidate module and moving its classes into `src/main/java`.  We then identified its dependencies and declared them in `build.gradle.kts`.  For packages that were not yet ready to become modules, we used Gradle’s `sourceSets` to separate them logically until we could resolve their cycles.  Each time we extracted a module, we ensured that existing builds and tests remained green.  Following the Gradle migration guidance, we kept the Ant build alongside the Gradle build until both produced identical artifacts【495392568104378†L356-L371】.  We also verified that the new modules did not break the IDE experience.  As modules stabilised, we deprecated the corresponding packages in the monolithic project and documented the migration steps for the next team.  Gradual conversion allowed us to learn and adapt as we went.

#### Section 18: Case Study – Consolidating UI Modules

Our legacy system included separate **UI-Admin** and **UI-Customer** modules, each with its own controllers and templates.  Initially we planned to extract them into separate Gradle projects.  However, our coupling analysis showed that both modules reused a large portion of the same views and shared common controllers.  Nearly every commit touched both modules.  Using the principles from the Spring Modulith workshop, we decided to merge these modules【555877237476569†L87-L91】.  We created a single **UI** module with subpackages for administrative and customer functions.  To maintain separation, we defined interfaces for pages and actions that were implemented by specific subpackages.  The merger simplified navigation, reduced duplication and made it easier to apply consistent styling.  It also highlighted that our module boundaries should follow user journeys rather than arbitrary categories.  The lesson: do not hesitate to merge modules if analysis reveals they are naturally one.

#### Section 19: Case Study – Extracting Domain Services

Another example involved the **Payments** component, which handled everything from credit card authorisation to invoice generation and email notifications.  This entanglement violated the rule that a module should have a single responsibility.  We applied DDD and identified three bounded contexts: **Billing**, **Accounting** and **Notifications**.  Each context became its own module.  We used the inversion of control technique described earlier【555877237476569†L63-L66】 to decouple these modules: `Billing` defined interfaces for charging a card; `Accounting` implemented these interfaces and posted transactions to the ledger; `Notifications` subscribed to events and sent receipts.  The extraction uncovered hidden dependencies—such as a call from `Billing` to `Notifications` to update the user’s payment profile—which we replaced with events【555877237476569†L87-L91】.  Post‑extraction, each module could be built, tested and released independently.  Over time, these modules may evolve into separate services, but for now they live within the monolith, providing clear boundaries and ownership.

#### Section 20: Summary and Next Steps

In this chapter we re‑examined what it means for code to be a module.  We distinguished between mere namespaces and true modules that encapsulate responsibilities and expose clear interfaces【289604801981996†L83-L114】.  We contrasted layered monoliths with package‑by‑feature and package‑by‑component structures【351796604151849†L56-L63】【681326574613439†L70-L92】, emphasising that no single approach fits all cases.  Drawing on Domain‑Driven Design and microservices principles, we grouped modules around business capabilities【567388280390324†L64-L74】 and identified bounded contexts to delineate ownership.  To break cycles and reduce coupling we inverted dependencies, extracted shared utilities and used events and orchestrators【555877237476569†L55-L59】【555877237476569†L63-L66】【555877237476569†L87-L91】.  We explored how Gradle’s `sourceSets` and clear public interfaces reinforce boundaries, and we discussed cross‑cutting concerns, feature flags and testing strategies.

This modularisation journey is iterative.  Modules may need to be merged or split as insights emerge, and metrics of cohesion and coupling should guide decisions.  In the next chapter we will delve into dependency configurations—how `api`, `implementation`, `compileOnly` and `runtimeOnly` map to the worlds of compilation, running and publishing.  We will learn how to align Gradle’s configurations with Maven scopes and ensure that our modules can be compiled, run and consumed consistently across environments.
# Gradle by Example: Modularising a Legacy Java Application

This book chronicles the journey of refactoring a sprawling Java monolith into a modular Gradle build.  Each chapter corresponds to a phase of the migration and includes roughly twenty sections to give the topic adequate depth.  The structure of the book mirrors the **“Refactoring the Giant: Modularizing Legacy Java Applications in the Gradle Era”** report.  The following content represents the beginning of the manuscript; more sections will be added in subsequent iterations.

## Part I – Understanding the Monolith

The first part of the book examines the legacy system we are starting with.  We explore how years of shortcuts created a *big ball of mud*【289604801981996†L83-L114】, why the codebase has no real modular structure, and what challenges that poses for modernization.  This part sets the baseline from which we will measure progress throughout the refactoring.

### Chapter 1: The Labyrinth – Inheriting a Tightly‑Coupled Monolith

Inheriting a legacy monolith can feel like stepping into a labyrinth.  What appears to be a collection of “modules” often turns out to be a single tangled codebase.  In this chapter we examine why the code looks the way it does, how small hacks and short‑cuts accumulate over time and what it means for your migration to Gradle.

#### Section 1: The Legacy Codebase – An Overview

Most legacy projects evolve without a coherent architectural vision, accruing technical debt as developers prioritise immediate fixes over long‑term maintainability.  The resulting codebase is often referred to as a *big ball of mud* — an unstructured system where components are tightly coupled and dependencies are intertwined【289604801981996†L83-L114】.  Over years of hurried development, short‑cuts taken to meet deadlines accumulate.  Documentation is sparse or outdated, and the business logic spreads across multiple packages with little respect for boundaries【289604801981996†L123-L142】.  When new developers join the team, they face a steep learning curve because the system has no clear entry points or separation of concerns.

In our case study, the “monolith” pretended to be ~30 subprojects checked into an SVN repository.  Each subproject had its own folder and build script, but at runtime they acted as a single application.  A common launcher pulled in code from every “module” to assemble one giant Swing UI.  Developers patched the classpath by hand to get the right jars on the classpath, and all compiled classes were dumped into a single `bin` directory.  If you wanted to run the application locally, you had to import dozens of Ant targets and manually set up the environment.  The lack of clear boundaries made it nearly impossible to refactor without breaking something, and even minor changes required understanding the whole system.

#### Section 2: Monolith in Disguise – False Modules

Although the repository contained many folders, these so‑called modules were only namespaces.  The *big ball of mud* anti‑pattern teaches us that when a system lacks a clear modular structure, components become tightly coupled and dependencies cross cut every layer【289604801981996†L123-L142】.  That was exactly the situation: business logic was scattered across “client‑common”, “core”, “ui” and “data” projects with no clear ownership.  Utility classes were duplicated in several modules, and cyclic dependencies forced developers to order their builds manually.  Refactoring a class often meant hunting down dozens of dependants scattered across the codebase.

This false modularity also hid the true scale of the system.  On paper each module had its own `build.xml`, but the main application pulled everything together at runtime by reading the `classpath` entries from each project and concatenating them into a single path.  There was no separate deployment of modules; everything was shipped as one jar or exploded directory.  Developers added new modules by copying an existing folder and tweaking the package names.  Over time this duplication increased maintenance costs and created more coupling.  Recognising these modules as purely organisational is the first step in designing real separation.  In later chapters we will redefine modules based on business capabilities and extract them into independent Gradle projects.

#### Section 3: Classpath Magic and Eclipse Hacks

To keep the monolith running, teams resorted to classpath tricks and IDE hacks.  Eclipse launchers were used to assemble a gigantic classpath that included every compiled class from all modules.  Each developer had a slightly different `.classpath` file that referenced local directories and third‑party jars checked into SVN.  When something broke, the fix was often to reorder entries in the launch configuration or manually add missing jars.  Over time these hand‑maintained classpaths became brittle and only worked on certain machines.

Another problem stemmed from the Ant build itself: the `build.xml` for each module produced outputs in a common `bin` directory, overwriting classes with the same fully qualified name.  Developers configured Eclipse to point at this shared `bin` directory so the IDE could run the application.  Because there was no concept of `api` versus `implementation` dependencies, everything was on the compile classpath all the time.  This violated the principle of minimal coupling and made it impossible to enforce encapsulation.  When moving to Gradle we will use modern configurations (`implementation`, `api`, `compileOnly`, etc.) to express dependencies precisely and rely on the IDE integration provided by the Java plugin.  Doing so will eliminate the need for custom launchers and manual `.classpath` files.

#### Section 4: Ant Build Practices in the Old Era

The original build system was based on Apache Ant.  Ant offered immense flexibility but no conventions, so each project defined its own targets to compile, package and deploy code.  Migrating such builds to Gradle is challenging because there is no standard Ant layout【495392568104378†L301-L339】.  The Gradle documentation recommends two main approaches: importing the existing Ant build using `ant.importBuild()`, which converts Ant targets to Gradle tasks but preserves the original structure, or rewriting the build to adopt idiomatic Gradle conventions【495392568104378†L320-L341】.  Importing an Ant build is quick and ensures the Gradle build produces the same artifacts, but you lose many advantages of Gradle’s dependency management and plugin ecosystem【495392568104378†L324-L332】.  Starting fresh with Gradle plugins (for example, the Java or Java Library plugins) requires more up‑front work but results in a simpler, easier‑maintained build【495392568104378†L335-L347】.

The guideline from the Gradle user manual is to keep the old Ant build and the new Gradle build side by side until you can verify that the outputs are identical【495392568104378†L356-L371】.  Teams are encouraged to develop a mechanism to compare artifacts, decide whether the project should be a multi‑project build, and evaluate which plugins to apply【495392568104378†L356-L385】.  Only after confirming functional equivalence should you migrate directory structures and adopt Gradle conventions【495392568104378†L391-L425】.  In the context of our legacy monolith, these recommendations mean we will initially import the Ant targets into Gradle so we can run and debug the application as before.  Then we will gradually refactor modules into separate projects, eliminate redundant tasks and adopt dependency configurations that better express the relationships between components.

#### Section 5: SVN Repository Structure

The source code for our monolithic application lived in a Subversion (SVN) repository.  Subversion uses a simple yet powerful concept of a *project root* that contains three directories: `/trunk`, `/branches` and `/tags`【362435736783276†L11-L14】.  The trunk holds the main line of development; branches are used for work on features or experiments; tags capture snapshots of a particular revision for releases or milestones.  A well‑structured repository helps teams manage releases and concurrent development.  However, our repository layout deviated from this best practice.  Each “module” was under its own top‑level directory, and there was no central trunk.  Branches were rarely used, which meant that developers committed directly to the main line, leading to instability and conflicts.

Having multiple project roots also meant we could not easily snapshot the whole application at once.  Tags were created per module, so reconstructing a release required checking out different tags and piecing them together.  This ad‑hoc structure complicated merges and hindered the migration to Gradle.  A later chapter will show how we reorganize the repository into a single project root with clearly defined trunk, branches and tags, enabling us to manage versions of the entire system.

#### Section 6: Intermodule Dependencies and Cycles

One of the most problematic aspects of our legacy application was the tangled web of dependencies between modules.  Cyclic dependencies were rampant — module A compiled against module B, while module B depended back on module A.  Breaking these cycles is essential for modularization.  Tools such as Spring Modulith can help by verifying that there are no dependency cycles between modules and letting you specify which dependencies are allowed【555877237476569†L15-L29】.  If your application is already organized into separate projects, Maven or Gradle will enforce cycles at the build level, but in a monolithic codebase cycles may exist hidden within packages【555877237476569†L30-L39】.

Breaking a dependency cycle often involves refactoring.  One strategy is to move shared functionality into a common module or library that both dependent modules can use【555877237476569†L55-L59】.  Another approach is to invert the dependency by introducing an interface in the consuming module and providing an implementation in the supplying module【555877237476569†L61-L66】.  Events or messaging patterns can also decouple modules, although this introduces asynchronous behaviour and may require an orchestrator【555877237476569†L69-L99】.  In our migration, we will analyse each pair of modules to decide whether to extract shared interfaces, merge highly coupled modules or design a new orchestrator module that centralises cross‑cutting concerns.  Addressing cycles early will make subsequent Gradle builds simpler and support incremental extraction of services.

#### Section 7: Duplicate and Overlapping Code

As the codebase grew, developers often copied and pasted functionality rather than extracting reusable components.  This *code duplication* means that multiple segments of code perform the same or similar tasks【81784860099387†L775-L779】.  Reasons include speed (copying is quicker than designing a reusable API), lack of awareness of existing functionality and teams working in silos【81784860099387†L780-L785】.  Duplicated code increases the maintenance burden because changes have to be replicated across several locations【81784860099387†L789-L799】.  Bugs fixed in one copy may persist in another, leading to inconsistent behaviour and wasted effort【81784860099387†L789-L799】.

To address duplication, we first need to identify it.  Static analysis tools can detect similar code blocks across the project【81784860099387†L800-L804】.  Once duplication is found, we can consolidate code into shared modules or utility classes.  Gradle modules encourage code reuse because you can declare a dependency on a shared module and avoid copying code.  Regular code reviews and refactoring sessions help reduce duplication over time【81784860099387†L807-L813】.  In later chapters we will extract common code into a `core-utils` project and gradually delete the copies.  Reducing duplication not only makes the codebase smaller but also clarifies the boundaries between modules.

#### Section 8: Technical Debt Accumulation

Technical debt arises when short‑term shortcuts are taken to deliver features quickly【81784860099387†L166-L170】.  Legacy code is a prime example: outdated code that still runs the business but no longer aligns with current best practices【81784860099387†L173-L184】.  Reasons for legacy code include evolving technology, lack of regular refactoring and the departure of original developers, which makes the code hard to understand【81784860099387†L180-L186】.  Accumulated technical debt leads to serious consequences: maintenance becomes more costly, integrating new technologies is difficult and the system becomes less agile【81784860099387†L189-L198】.  Developers may avoid touching certain parts of the code for fear of breaking it, so the debt grows further.

In our monolith, debt has accumulated over two decades.  Hard‑coded values, outdated libraries, lack of documentation and inconsistent coding styles are common【81784860099387†L166-L198】.  The migration to Gradle offers an opportunity to pay down this debt.  We will invest time in refactoring: restructuring code without changing its behaviour, improving documentation, introducing automated tests and replacing deprecated libraries.  Regularly scheduled maintenance cycles and a culture of continuous improvement are necessary to prevent debt from growing again【81784860099387†L208-L212】.

#### Section 9: Maintenance Challenges and Pain Points

The consequences of technical debt manifest in daily maintenance challenges.  As the Brainhub guide notes, legacy code requires more time and resources to maintain and hinders future development【81784860099387†L189-L198】.  Developers spend significant effort understanding the code before making changes, and fear of breaking something discourages refactoring【81784860099387†L180-L186】.  Because the monolith lacks unit tests and has many hidden dependencies, any change can have unforeseen side effects.  Even a small feature or bug fix requires navigating through layers of tightly coupled components.

Another pain point is that the build and deployment process is fragile.  Since the Ant scripts produce a single giant artifact, a failure in one module delays the release of everything.  The absence of modular boundaries means you cannot ship independent updates, and the time to build and test increases.  Team members often step on each other’s toes because everyone works in the same codebase.  These challenges sap productivity and make developers reluctant to innovate.  Migrating to Gradle and modularising the application will allow us to isolate changes, shorten build times and make maintenance more predictable.

#### Section 10: Impact on Developer Productivity

All these issues—false modules, cyclic dependencies, duplicated code and technical debt—take a toll on developer productivity.  When the codebase is a tangled mess, even simple tasks require navigating complex interactions.  As technical debt grows, the system becomes harder to understand, leading to more time spent reading code than writing it【81784860099387†L189-L198】.  Developers must reinvent wheels because existing functionality is hidden or duplicated.  Build times increase as the monolith grows, and debugging requires instrumenting many interdependent components.

Poor tooling exacerbates the problem.  Without proper IDE integration, developers must maintain custom classpaths and launch configurations.  Slow feedback loops discourage experimentation, and the risk of breaking something reduces confidence.  By investing in a modern build system (Gradle), introducing modular projects and improving documentation and testing, we aim to restore developer productivity.  The migration will allow teams to focus on business value rather than wrestling with the build.

#### Section 11: Business Reasons for Refactoring

Refactoring a legacy system is not just a technical exercise – it is driven by concrete business needs.  Legacy applications often become bottlenecks for innovation because they are inefficient, insecure, costly to maintain and difficult to integrate with modern systems.  The Kissflow overview of legacy modernization notes that outdated systems suffer from security vulnerabilities, expensive maintenance, inefficiency and incompatibility【128542396033773†L234-L277】.  These issues translate into higher operational costs, slower response to market changes and increased risk of regulatory non‑compliance.  Organizations continuing to run on outdated platforms can lose productivity due to long load times and difficulty integrating with new tools【128542396033773†L262-L273】.  Modernization is therefore an investment in future agility: updating the architecture reduces maintenance costs, improves performance and makes it easier to comply with evolving regulations【128542396033773†L300-L303】.

From a business perspective, modernization unlocks new opportunities.  When applications are modular and use current technologies, teams can release features faster, integrate with cloud services and respond to customer feedback more quickly.  Enhanced security reduces the risk of breaches and associated fines.  Reduced maintenance frees up budget for innovation.  Although modernization requires upfront effort, the long‑term pay‑off is improved competitiveness and the ability to adapt to future market demands.  This section frames the rationale for the migration described in the rest of the book.

#### Section 12: Goals of Modularization

With the business case established, we define what success looks like.  Modularization divides a complex system into independent parts so that each can be developed, tested and deployed separately.  GeeksforGeeks explains that the advantages of modularization include making the system easier to understand, simplifying maintenance and promoting reuse【952315893773671†L151-L159】.  By reducing coupling and increasing cohesion, you minimise the ripple effects of change and can reason about components in isolation.  The goal is to transform the monolith into a set of modules with clear responsibilities and well‑defined interfaces.

This modular architecture also provides a pragmatic compromise between a monolith and microservices.  A blog on modular software architecture notes that modular monoliths let teams organise an application into distinct, independent modules while maintaining a single deployable unit【968675367308057†L552-L579】.  Such architectures improve long‑term maintainability and operational efficiency because each module can be modified without affecting the rest.  For our project, goals include creating modules aligned with business capabilities, enabling parallel development, reducing build times and paving the way for future extraction into services if needed.  Achieving these goals requires careful analysis of dependencies and thoughtful design of module boundaries, which we begin in the following sections.

#### Section 13: Preparing the Team for Change

Moving from a big ball of mud to a modular Gradle build is as much a people challenge as it is a technical one.  Resistance to change is natural: employees may be comfortable with familiar workflows and worry that new tools will make their jobs harder.  The Whatfix migration guide observes that end‑users tend to resist changes that disrupt their routines and may fear they cannot adapt to new systems【324595831095222†L203-L213】.  If employees feel they have little input into the process, they may oppose the migration out of a sense of lost autonomy【324595831095222†L214-L216】.

To prepare the team, leaders must invest in training and communication.  The same article highlights the importance of providing context‑specific training and self‑help resources【324595831095222†L217-L235】.  A separate piece on human barriers to legacy migration notes that fear of the unknown and a desire to maintain control are common motivators for resistance【840625522851828†L122-L147】.  Addressing these concerns requires involving developers in planning, explaining the benefits of Gradle, offering workshops on build scripts and setting up sandbox environments for experimentation.  Establishing two‑way communication channels ensures that feedback is heard and that team members feel valued.  By building cross‑functional migration teams and empowering individuals through training, you can turn scepticism into enthusiasm.

#### Section 14: Tools and Technology Landscape

Before diving into refactoring, it is important to survey the tool ecosystem that will support the migration.  At the heart of the new build is Gradle, a modern build system that offers declarative dependency management, caching and IDE integration.  The Gradle user manual outlines two primary strategies for migrating from Ant: importing existing Ant builds using `ant.importBuild()` to replicate current behaviour or rewriting the build using idiomatic Gradle plugins【495392568104378†L320-L341】.  Initially importing the Ant build allows you to keep the old and new builds side by side while verifying that the outputs remain identical【495392568104378†L356-L371】.

Additional tools play supporting roles.  IDE plugins like Buildship provide tight integration with Gradle, eliminating the need for custom classpath hacks.  Version control systems like SVN will need to be migrated or bridged; we will reorganise the repository to follow the recommended `/trunk`, `/branches` and `/tags` layout【362435736783276†L11-L14】.  Dependency analysis tools help detect cyclic dependencies and unused libraries.  Static analysis tools identify duplicate code.  Together, these technologies form a stack that enables continuous builds, automated testing and improved developer productivity.  Choosing the right combination of tools ensures that the migration effort delivers long‑term benefits rather than simply replacing one pain point with another.

#### Section 15: Stakeholder Communication

Successful modernization requires buy‑in from people beyond the development team.  Stakeholders include project sponsors, business owners, QA, operations and end‑users.  The Whatfix article recommends assembling a migration team that includes executives, project managers, technical leads, security officers, QA leads and representatives from each affected business unit【324595831095222†L310-L329】.  Clearly defining roles and establishing regular communication channels, such as weekly meetings and progress reports, ensures alignment and transparency【324595831095222†L330-L334】.

Effective communication also means articulating the benefits of modularization in terms stakeholders care about: reduced costs, faster releases, improved security and compliance.  Provide updates on progress and explain how milestones relate to business goals.  Encourage feedback and address concerns promptly to maintain trust.  When stakeholders understand the rationale and see incremental improvements, they are more likely to support the project through inevitable challenges.

#### Section 16: Assessing Build Practices

A critical early step is to assess the current build practices to identify gaps and inform the migration plan.  The Gradle manual advises keeping the existing Ant build alongside the new Gradle build while you verify that the outputs are identical【495392568104378†L356-L371】.  Teams should develop mechanisms—such as automated comparisons of jars or checksums—to ensure functional equivalence.  During this period you can evaluate whether the project should be a single multi‑project build and decide which Gradle plugins are appropriate【495392568104378†L356-L385】.

Assessment also involves understanding how builds are orchestrated across modules.  Investigate how dependencies are declared, where compiled classes are output, and how artifacts are packaged.  Identify custom scripts, manual steps and brittle launch configurations that need to be eliminated.  Document the pain points—such as long build times, cyclical dependencies and inconsistent environments—to prioritise what to address first.  This baseline analysis informs the roadmap for migrating tasks to Gradle, adopting proper dependency configurations and eventually decommissioning the Ant scripts.

#### Section 17: Documenting Existing Architecture

Legacy projects frequently suffer from poor or missing documentation.  ModLogix notes that legacy code often relies on a few developers who understand it, and when these people leave the organization the system becomes almost impossible to maintain【619318602968355†L117-L124】.  Without documentation, teams underestimate workloads and struggle to identify what each component does【619318602968355†L126-L129】.  Documenting the existing architecture is therefore essential before making changes.

Start by writing down what the application does and which business problems it solves【619318602968355†L145-L148】.  Map how components interact, including data flows and external integrations.  ModLogix suggests creating diagrams to visualise system phases and component relationships【619318602968355†L149-L158】.  Documenting these details provides a blueprint that new developers can follow and helps identify which parts of the system can be modularised.  It also surfaces dependencies and hidden coupling that might otherwise derail the migration.  Treat documentation as a living artifact—update it as you refactor so the architecture remains transparent.

#### Section 18: Identifying Quick Wins

While the end goal is a fully modularized system, delivering early wins builds momentum and confidence.  However, Martin Fowler’s article on patterns of legacy displacement warns that seeking low‑disruption “quick win” solutions without a broader plan can become an anti‑pattern【646641713727892†L368-L372】.  A more sustainable approach is to break the problem into smaller parts by finding seams in the existing architecture【646641713727892†L434-L443】.  By understanding how the system maps to business capabilities, you can extract individual modules that deliver value on their own【646641713727892†L439-L443】.

In practice, quick wins for our project might include extracting a utility library used across the system into its own Gradle module, adding automated tests around critical components, or reorganizing the SVN repository into a proper trunk/branches/tags structure.  These tasks reduce duplication, reveal dependencies and lay the groundwork for larger refactors.  Use tools like event storming and domain mapping to identify natural boundaries and start with modules that have few dependencies or are poorly coupled【646641713727892†L460-L479】.  Each small victory demonstrates progress to stakeholders and provides lessons for subsequent, more complex extractions.

#### Section 19: Setting Up a Baseline for Comparison

To measure progress, you need a baseline.  Begin by capturing quantitative metrics about the current system: build times, startup times, memory consumption and defect rates.  Establish test suites that verify functional behaviour so you can ensure the Gradle build produces the same outputs as the Ant build.  The Gradle manual emphasises keeping both builds side by side and comparing artifacts until you are confident of equivalence【495392568104378†L356-L371】.  This baseline helps detect regressions and quantifies improvements as modules are extracted and build logic is simplified.

Qualitative metrics are also important.  Survey developers about pain points and track how long common tasks take.  Document recurring issues such as missing dependencies or broken classpaths.  After migrating a module, re‑measure these metrics to demonstrate gains.  Setting up this baseline not only guides prioritisation but also provides evidence to stakeholders that the migration is delivering value.

#### Section 20: Summary and Chapter Takeaways

This first chapter explored the labyrinth of a tightly coupled legacy monolith.  We saw how a sprawling codebase disguised as modules created a *big ball of mud*【289604801981996†L83-L114】, how classpath hacks and Ant build practices allowed the system to limp along, and how the SVN repository structure hindered version control.  We examined the technical challenges—cyclic dependencies, duplicate code, technical debt and poor developer productivity—and their impact on the team【81784860099387†L189-L198】.  Then we stepped back to look at the business reasons for refactoring: outdated systems are insecure, inefficient and costly【128542396033773†L234-L277】, while modernization promises improved performance and reduced costs【128542396033773†L300-L303】.

With the problem defined, we articulated goals for modularization—easier understanding, maintainability and reuse【952315893773671†L151-L159】—and surveyed the tools and technologies that will enable the migration【495392568104378†L320-L341】.  We emphasized preparing the team through training and communication【324595831095222†L203-L235】, involving stakeholders【324595831095222†L310-L334】, assessing current build practices【495392568104378†L356-L385】 and documenting the existing architecture【619318602968355†L117-L158】.  Finally, we outlined strategies for quick wins【646641713727892†L434-L443】 and the importance of establishing a baseline for comparison【495392568104378†L356-L371】.  In the chapters that follow, we will build on this foundation, diving deeper into Gradle migration, modular design and deployment practices.  The labyrinth may be complex, but with a clear roadmap and disciplined approach, we can navigate it successfully.