## Part IX – Refactoring Without Rewriting – Patterns for Gradual Modularity

Massive rewrites are risky and rarely feasible for long‑lived enterprise systems.  The goal of this part of the book is to show how you can modularize and modernize a legacy Java application incrementally without shutting it down.  Rather than throwing the entire monolith away, you will learn how to identify seams in the system, extract functionality into independent modules, and gradually encapsulate old code behind stable interfaces.  We will cover established patterns like the Strangler fig approach, anti‑corruption layers and adapters, techniques for using automated tests and static analysis to guide refactoring, and advice on balancing refactoring work with day‑to‑day feature development.  By the end of this chapter, you will have a pragmatic roadmap for improving the architecture of a legacy system one step at a time.

### Chapter 10: Refactoring Without Rewriting – Patterns for Gradual Modularity

#### Section 1: The Strangler Pattern: Gradual Encapsulation

One of the most widely recognised patterns for migrating legacy systems is the **Strangler Fig** pattern.  The term originates from a fig tree that gradually envelops and replaces its host.  In software, it means placing a façade in front of the old system so that new requests are routed through a proxy.  The Azure Architecture Center explains that the façade allows the client to interact with both the legacy system and new services; as migration progresses, the façade gradually shifts requests from the old system to the new one【514646192745409†L138-L184】.  This incremental approach lets you replace specific pieces of functionality without breaking the existing application and reduces the risk of disruption【514646192745409†L165-L176】.  The key to using the Strangler pattern effectively is to identify seams—interfaces or endpoints—where new modules can intercept calls, implement a small slice of functionality in the new module, and then wire the façade to direct those calls to the new component.  Over time, the monolithic code shrinks as more functions are strangled, and eventually the façade can be removed entirely.  In our migration we adopted this pattern by introducing an API gateway module that handles requests and forwards them either to legacy controllers or to newly extracted services.

#### Section 2: Identifying Candidate Modules for Extraction

Before you can extract modules, you need to decide which parts of the monolith offer the best return on investment when separated.  Look for bounded contexts and cohesive feature areas that can stand on their own.  Candidates often have well‑defined inputs and outputs—for example, a reporting subsystem, a payments service, or user management.  The microservices community recommends starting with new functionality: implementing significant new features as services demonstrates the value of modularization【165920568660542†L20-L25】.  Another clue is areas of code that change at a different cadence from the rest of the application; isolating these allows independent evolution.  Use dependency analysis tools (Gradle’s `dependencyInsight` or `projectReport`) to visualise how classes depend on each other and identify self‑contained subgraphs【814383946455057†L302-L311】.  Once you have identified a candidate, perform a quick feasibility assessment: check that the data it needs is accessible via well‑defined interfaces and that extracting it will not introduce unacceptable latency.  Selecting the right candidates early on builds momentum for the refactoring effort.

#### Section 3: Creating Independent Modules One at a Time

After choosing a candidate, the extraction process should be deliberately incremental.  Start by creating a new Gradle subproject representing the module.  Define its public API—typically interfaces or REST controllers—and wire the existing code to call those interfaces instead of directly accessing internal classes.  Initially the module may simply delegate to the existing monolithic implementation.  Once the boundaries are established, move the implementation classes into the new module one package at a time.  Compile and test frequently to ensure nothing breaks.  Gradle’s multi‑project build makes it easy to build just the module you are working on using `gradle :my-module:build` so that you get fast feedback.  By integrating tests and continuous integration, you maintain confidence that each small extraction behaves identically to the monolithic version.  Over time, the monolith becomes smaller and the module becomes self‑sufficient, with its own dependencies and release lifecycle.  Resist the temptation to extract multiple modules simultaneously; focusing on one area reduces complexity and isolates problems.

#### Section 4: Using Anti‑Corruption Layers and Adapters

During gradual migration, it is common to have new modules interact with legacy subsystems.  Eric Evans’ **anti‑corruption layer** pattern provides a disciplined way to manage these interactions.  An anti‑corruption layer sits between two subsystems and translates calls and data structures【504902233893626†L115-L149】.  The Azure pattern guide notes that this layer shields the new module from the legacy system’s convoluted data schemas or obsolete APIs, allowing the new code to remain clean【504902233893626†L129-L149】.  When implementing this pattern, define domain‑specific interfaces in the new module and implement them in an adapter that uses legacy services or data.  Map the data into the new module’s representation and perform validation or transformation as needed.  Consider implementing anti‑corruption layers as separate services or components so they can be reused across modules.  Keep them thin and free of business logic to avoid creating a new source of coupling.  In our refactoring, we created adapters for the legacy database access layer that converted raw JDBC calls into repository interfaces consumed by the new modules.  This protected the new code from SQL dialect differences and made it easier to test.

#### Section 5: Integrating New Modules into the Monolith

Once a new module is implemented, it must work seamlessly within the existing application.  The façade or API gateway used for the Strangler pattern routes requests to either the new module or the old code.  Inside the monolith, you may need to register the new module’s services with the dependency injection container or messaging infrastructure.  Ensure that configuration properties, Spring beans and classpath entries are set up correctly.  Use Gradle’s `api` and `implementation` configurations to control which modules expose their APIs to others and prevent accidental leaks of internal dependencies【21875673602686†L325-L361】.  When integrating front‑end modules, update the UI to call the new backend service through the gateway.  Because the new module may access shared data, you might have to migrate part of the database schema or replicate it; we discuss data migration later.  Plan for incremental rollout: deploy the module behind a feature flag or to a subset of users, monitor behaviour and gather feedback before directing all traffic to it.  Integration is often the hardest part of modularisation, so allocate time for testing end‑to‑end flows.

#### Section 6: Maintaining Backward Compatibility

Throughout the migration, the legacy system continues to serve existing clients.  To avoid breaking them, new modules must be backward compatible with existing APIs, data formats and behaviours.  This means adding new endpoints rather than modifying old ones, using versioned APIs, and ensuring that database changes are additive (e.g., adding columns rather than renaming them).  When migrating UI code, keep the contract between the server and the front end stable until the new server is ready.  Use contract tests to verify that new modules respond as expected when old clients call them.  If you are using message brokers, maintain topic and message format stability until all consumers have been upgraded.  Gradle’s test fixtures and integration tests can help you codify these guarantees.  Also, maintain fallback paths: if a new module fails or is incomplete, the façade should route requests back to the monolith.  Backward compatibility preserves trust in the system and allows you to release improvements frequently without a big‑bang cutover.

#### Section 7: Creating Test Suites for Refactoring

A comprehensive test suite is your safety net during incremental refactoring.  At a minimum, ensure you have unit tests for critical business logic, integration tests for module boundaries, and end‑to‑end tests for user flows.  Use test coverage tools to identify untested areas and focus on those before extracting code.  When new modules are introduced, write tests against their public APIs, not their internal classes.  You can execute tests quickly using Gradle’s `test` task in continuous build mode (`--continuous`) to get immediate feedback.  In IDEs like IntelliJ or Eclipse Buildship, choose the Gradle test runner to ensure results match CI and the command line【758756572275466†L20-L69】.  Consider contract testing frameworks such as Pact or Spring Cloud Contract to validate interactions between modules and the monolith.  Running tests early and often allows you to refactor fearlessly, catch regressions promptly and maintain confidence in your incremental changes.

#### Section 8: Using Static Analysis Tools for Architecture Rules

Static analysis and architecture enforcement tools help prevent the reintroduction of undesirable dependencies.  Gradle’s Checkstyle, PMD and SpotBugs plugins can enforce code conventions and detect common bugs.  More importantly, tools like ArchUnit allow you to specify architecture rules in code: for example, preventing classes in the `domain` package from depending on `web` controllers.  By running these checks in your CI pipeline and failing the build when violations occur, you maintain module boundaries and avoid regressions.  The BairesDev article on static code analysis notes that static analysis identifies security risks and enforces coding standards, but should be paired with dynamic testing to catch concurrency issues【364352143673742†L151-L176】.  ArchUnit and similar tools extend this idea to architectural constraints, helping teams encode decisions about which modules may depend on which.  In Gradle, you can integrate ArchUnit into the test suite or write a custom plugin that fails the build if forbidden dependencies are detected.

#### Section 9: Generating Dependency Graphs for Guidance

Visualising the dependencies in your project makes it easier to plan refactorings.  Gradle’s built‑in `dependencies` and `dependencyInsight` tasks show the dependency tree for each configuration and highlight where versions are selected【814383946455057†L302-L311】【814383946455057†L347-L356】.  To generate more comprehensive graphs, use the `project-report` plugin to create HTML reports that list project dependencies and tasks【138700670089114†L288-L330】.  Third‑party tools like Gradle Dependency Graph Generator or `graphviz` scripts can render the tree as diagrams.  When planning an extraction, generate a graph for the current module and identify incoming and outgoing edges.  Aim to cut edges by introducing interfaces or moving shared code into a separate module.  Repeat the process as you extract parts of the monolith to see how the overall graph simplifies over time.  These visualisations help communicate progress to stakeholders and validate that coupling is decreasing.

#### Section 10: Running Migration Workshops for Teams

Refactoring a large codebase is a team sport.  Migration workshops provide a structured way to align developers, architects and product owners on goals and techniques.  The Whatfix article on legacy system migration suggests forming a cross‑functional team with a project sponsor, technical leads, security officers and representatives from business domains, and holding regular communication sessions to keep everyone informed【324595831095222†L310-L334】.  In our experience, workshops can take the form of half‑day sessions where the team reviews candidate modules, discusses extraction strategies, and practices using tools such as ArchUnit or dependency graphs.  Live coding exercises help team members become comfortable with new patterns and encourage knowledge sharing.  Invite stakeholders to observe demos of extracted modules to build confidence in the process.  Regular workshops keep momentum and provide a forum for raising concerns before they become blockers.

#### Section 11: Balancing Refactoring with New Feature Development

It can be tempting to postpone new features until the architecture is perfect, but this is rarely viable.  Continuous integration advocates merging changes frequently and using automated tests to detect regressions【109171532237068†L115-L120】.  Apply the same principle to refactoring: allocate a portion of each sprint to modularization work while still delivering user‑visible features.  Use feature toggles to hide incomplete modules and release them gradually.  Track technical debt in your backlog and prioritise it alongside features.  Communicate clearly with product management about the benefits of paying down debt—improved velocity, reduced defects and better maintainability.  Resist the urge to rush extraction efforts; quality matters more than speed.  Balancing new development with refactoring keeps stakeholders engaged and ensures that the system continues to evolve rather than stagnate.

#### Section 12: Documenting Technical Debt and Roadmaps

Documentation is often neglected in legacy systems, yet it is essential for successful modularisation.  The ModLogix guide to documenting legacy code advises mapping system interactions, creating diagrams and documenting dependencies to aid understanding【619318602968355†L117-L158】.  Start by recording the current architecture: identify modules, packages, shared databases and external systems.  Maintain a list of technical debt items—such as cyclic dependencies, shared state and outdated libraries—along with their potential impact and remediation plans.  Use tools like `architecture-as-code` to store architecture decisions in version control.  Create a roadmap that sequences module extractions, identifies prerequisites and notes how success will be measured.  Revisit the roadmap regularly and adjust it based on feedback and new discoveries.  Transparent documentation helps align the team and gives stakeholders confidence in the modularisation plan.

#### Section 13: Establishing Governance for Module Boundaries

Clear rules about which modules may depend on others are crucial for long‑term modularity.  The Spring Modulith toolkit enforces that module dependencies form a directed acyclic graph and lets you specify allowed dependencies【311016459650721†L93-L104】.  Adopt similar governance in your project: define layers (e.g., core, domain, infrastructure) and forbid dependencies from higher layers to lower layers.  Use architecture tests to codify these rules and automatically verify them during the build.  Encourage code reviews to check that new dependencies are justified and documented.  In a multi‑team environment, establish a technical steering committee to review requests for cross‑module dependencies and maintain a catalogue of shared services.  Governance should not be heavy‑handed, but it should provide guardrails that prevent regression into a tangled ball of dependencies.

#### Section 14: Assessing Microservices as a Next Step

As the monolith is decomposed into modules, the question arises: should you continue towards microservices?  Microservices.io notes that organizations often adopt the Strangler pattern and gradually migrate functionality from the monolith into services【165920568660542†L20-L25】.  Extracting new functionality as services is a good first step because it demonstrates business value and allows teams to build experience with distributed systems【165920568660542†L59-L64】.  However, microservices introduce complexity: distributed transactions, network latency, deployment orchestration and observability overhead.  Evaluate whether your team has the operational maturity and tooling to handle this complexity.  Consider starting with a modular monolith—separate modules within one process—and only splitting into separate processes when there is a strong need for independent scaling or deployment.  Many organizations find that a well‑structured modular monolith provides most of the benefits of microservices without the overhead.

#### Section 15: Migrating Data and State Across Modules

Extracting a module often requires untangling data.  Start by analysing which tables and entities belong to the module and whether they can be isolated into their own schema.  Use the façade from the Strangler pattern to route reads and writes while gradually migrating data.  You might implement event‑driven replication: when the legacy system writes to a table, publish an event that the new module consumes to build its own copy of the data.  During migration, the anti‑corruption layer translates between the old and new schemas【504902233893626†L146-L159】.  Once all reads and writes go through the new module, you can retire the shared table.  Be aware that database refactoring is often the hardest part of modularisation because it affects transactional integrity and requires careful coordination with DBAs.  Use database migration tools to manage schema changes and maintain backward compatibility.

#### Section 16: Handling Non‑Modularizable Legacy Code

Not all code can or should be extracted.  Some legacy components may be tightly coupled to the rest of the application or depend on technologies that are impractical to migrate.  In such cases, you can still improve modularity by encapsulating the legacy component behind a stable interface and isolating its usage.  Implement an anti‑corruption layer or adapter to shield the rest of the system from the legacy complexity【504902233893626†L146-L159】.  Document the reasons why the component cannot be extracted—perhaps it relies on a third‑party library with no maintained version or uses complex JNI bindings—and plan to revisit it in the future.  Meanwhile, focus your efforts on parts of the system that yield higher returns.  Accepting that some modules remain within the monolith helps avoid analysis paralysis.

#### Section 17: Case Study: Extracting a Reporting Module

In our migration project, the reporting subsystem was an ideal candidate for modularisation.  It generated PDFs and CSVs from database queries and had little coupling to the rest of the system.  We created a new Gradle subproject `reporting-module`, defined its public API with a service interface and moved the report generation classes into it.  Initially, the interface implementations delegated to the existing code, but over time we refactored them to use modern libraries and prepared SQL queries.  To migrate data, we created a separate schema for reporting tables and wrote migration scripts to copy historical data.  We introduced an anti‑corruption layer that converted domain entities into DTOs consumed by the report builder.  After thorough testing and a phased rollout, the new module served all report requests.  The legacy code was deleted and the monolith’s reporting package was removed from the classpath.  This success built confidence for extracting more modules.

#### Section 18: Celebrating Milestones and Maintaining Morale

Refactoring a legacy system can feel like a never‑ending journey.  Celebrating milestones helps maintain motivation.  When a module is successfully extracted, announce it in team meetings, write a blog post or hold a demo for stakeholders.  Recognise the contributors and highlight the benefits achieved, such as reduced build times or simplified dependencies.  The Daytona article on developer onboarding notes that effective onboarding improves retention and productivity because it gives new developers a sense of belonging and accomplishment【735336400211954†L66-L101】.  The same principle applies here: acknowledging achievements and providing learning opportunities keeps the team engaged.  Encourage pair programming and knowledge sharing so that modularisation expertise spreads across the team.  Celebrate small victories, but remain realistic about the remaining work.  Continuous positive reinforcement fosters a culture of continuous improvement.

#### Section 19: Future Plans and Evolving the Architecture

Modularisation is not a one‑time project; it is an ongoing process that evolves with business requirements and technology advances.  Keep refining the architecture by revisiting the roadmap, re‑evaluating module boundaries and incorporating new patterns and technologies.  Consider introducing more granular modules or adopting service‑oriented or microservice architectures when the operational maturity is in place【165920568660542†L20-L25】.  Investigate emerging tools for code analysis, dependency management and continuous delivery.  Explore the use of domain‑driven design to drive further improvements and align modules with business capabilities.  Set aside regular refactoring sprints to address technical debt and adapt to changes in the domain.  By continuously evolving the architecture, you ensure that the system remains flexible, maintainable and ready for future challenges.

#### Section 20: Summary and Lessons Learned

This chapter demonstrated that you can refactor a legacy Java application without rewriting it.  By adopting the Strangler pattern, identifying cohesive modules and extracting them incrementally, you can reduce risk and deliver value continuously.  Anti‑corruption layers shield new modules from legacy quirks and preserve clean design【504902233893626†L146-L159】.  Automated tests, static analysis and dependency visualisation help maintain boundaries and catch regressions【364352143673742†L151-L176】【814383946455057†L302-L311】.  Migration workshops and documentation keep the team aligned【324595831095222†L310-L334】【619318602968355†L117-L158】, while governance and roadmaps provide long‑term direction【311016459650721†L93-L104】.  Balancing refactoring with new development, celebrating milestones and planning for future evolution ensures sustained momentum【109171532237068†L115-L120】【735336400211954†L66-L101】.  In the next chapter we will explore how to conclude the migration by summarising our journey, assessing outcomes and laying out a path for continued improvement.